{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: notebook in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter) (7.2.0)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter) (7.16.4)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter) (6.29.4)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter) (8.1.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (8.24.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (8.6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (24.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipykernel->jupyter) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipywidgets->jupyter) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipywidgets->jupyter) (3.0.11)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-console->jupyter) (3.0.45)\n",
      "Requirement already satisfied: pygments in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-console->jupyter) (2.18.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbconvert->jupyter) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbconvert->jupyter) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbconvert->jupyter) (3.1.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbconvert->jupyter) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbconvert->jupyter) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbconvert->jupyter) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbconvert->jupyter) (1.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from notebook->jupyter) (2.14.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from notebook->jupyter) (2.27.2)\n",
      "Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from notebook->jupyter) (4.2.1)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from notebook->jupyter) (0.2.4)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (1.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (306)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (4.4.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.20.0)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (2.0.13)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter) (0.27.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter) (2.2.5)\n",
      "Requirement already satisfied: tomli>=1.2.2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter) (2.0.1)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter) (2.15.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter) (0.9.25)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter) (4.22.0)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter) (2.32.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.19.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.5)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->jupyter) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->jupyter) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->jupyter) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter) (0.18.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (0.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook->jupyter) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook->jupyter) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: fqdn in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.4)\n",
      "Requirement already satisfied: uri-template in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.9.0.20240316)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pyts in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from pyts) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from pyts) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from pyts) (1.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from pyts) (1.4.2)\n",
      "Requirement already satisfied: numba>=0.55.2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from pyts) (0.59.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from numba>=0.55.2->pyts) (0.42.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from scikit-learn>=1.2.0->pyts) (3.5.0)\n",
      "Name: ipython\n",
      "Version: 8.24.0\n",
      "Summary: IPython: Productive Interactive Computing\n",
      "Home-page: \n",
      "Author: The IPython Development Team\n",
      "Author-email: ipython-dev@python.org\n",
      "License: BSD-3-Clause\n",
      "Location: c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages\n",
      "Requires: colorama, decorator, exceptiongroup, jedi, matplotlib-inline, prompt-toolkit, pygments, stack-data, traitlets, typing-extensions\n",
      "Required-by: ipykernel, ipywidgets, jupyter-console\n",
      "Name: nbformat\n",
      "Version: 5.10.4\n",
      "Summary: The Jupyter Notebook format\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Jupyter Development Team <jupyter@googlegroups.com>\n",
      "License: BSD 3-Clause License\n",
      "        \n",
      "        - Copyright (c) 2001-2015, IPython Development Team\n",
      "        - Copyright (c) 2015-, Jupyter Development Team\n",
      "        \n",
      "        All rights reserved.\n",
      "        \n",
      "        Redistribution and use in source and binary forms, with or without\n",
      "        modification, are permitted provided that the following conditions are met:\n",
      "        \n",
      "        1. Redistributions of source code must retain the above copyright notice, this\n",
      "           list of conditions and the following disclaimer.\n",
      "        \n",
      "        2. Redistributions in binary form must reproduce the above copyright notice,\n",
      "           this list of conditions and the following disclaimer in the documentation\n",
      "           and/or other materials provided with the distribution.\n",
      "        \n",
      "        3. Neither the name of the copyright holder nor the names of its\n",
      "           contributors may be used to endorse or promote products derived from\n",
      "           this software without specific prior written permission.\n",
      "        \n",
      "        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "        AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "        IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "        DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "        FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "        DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "        SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "        CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "        OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "        OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "Location: c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages\n",
      "Requires: fastjsonschema, jsonschema, jupyter-core, traitlets\n",
      "Required-by: jupyter_server, nbclient, nbconvert\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: plotly in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (5.22.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from plotly) (8.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from plotly) (24.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: yfinance in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (0.2.40)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from yfinance) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from yfinance) (5.2.2)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from yfinance) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from yfinance) (2.4.4)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from yfinance) (3.17.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from requests>=2.31->yfinance) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from requests>=2.31->yfinance) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sergio\\aaa\\module_25\\req-act-25-3-project-jp\\.venv\\lib\\site-packages (from requests>=2.31->yfinance) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade jupyter\n",
    "!pip install pyts\n",
    "!pip install nbformat>=4.2.0\n",
    "!pip show ipython\n",
    "!pip show nbformat\n",
    "!pip install --upgrade plotly\n",
    "\n",
    "%pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "Device cuda:0\n",
      "cuda version 12.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import floor\n",
    "\n",
    "from pyts.image import GramianAngularField\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import yfinance as yf\n",
    "\n",
    "import torch.backends.cudnn\n",
    "\n",
    "#set gpu env\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\",device)\n",
    "print(\"cuda version\",torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close price time period\n",
    "start_date = '2021-10-01'\n",
    "end_date = '2023-12-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows 524\n",
      "Num rows for df Close col 524\n",
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "2021-10-01  650.000000  669.570007  648.065002  664.530029  664.530029   \n",
      "2021-10-04  663.989990  669.000000  645.729797  649.349976  649.349976   \n",
      "2021-10-05  657.909973  669.174988  652.072571  665.380005  665.380005   \n",
      "2021-10-06  654.039978  663.919983  643.219971  659.849976  659.849976   \n",
      "2021-10-07  670.929993  679.000000  663.280029  665.559998  665.559998   \n",
      "2021-10-08  670.594910  670.950012  663.090027  670.489990  670.489990   \n",
      "2021-10-11  678.000000  692.210022  666.469971  666.510010  666.510010   \n",
      "2021-10-12  663.349976  666.940002  650.289978  659.090027  659.090027   \n",
      "2021-10-13  659.090027  668.099976  647.849976  667.719971  667.719971   \n",
      "2021-10-14  676.760010  685.640015  673.000000  683.890015  683.890015   \n",
      "2021-10-15  694.059998  695.590027  679.159973  690.330017  690.330017   \n",
      "2021-10-18  685.993896  697.950012  685.993896  696.080017  696.080017   \n",
      "2021-10-19  698.380005  701.007324  692.384216  699.919983  699.919983   \n",
      "2021-10-20  698.700012  705.710022  697.000000  705.270020  705.270020   \n",
      "2021-10-21  704.289978  707.419983  693.710022  705.140015  705.140015   \n",
      "2021-10-22  720.000000  759.020020  720.000000  753.119995  753.119995   \n",
      "2021-10-25  755.320007  757.659973  745.465027  746.010010  746.010010   \n",
      "2021-10-26  747.340027  751.840027  740.030029  745.320007  745.320007   \n",
      "2021-10-27  738.539978  738.539978  706.989990  708.150024  708.150024   \n",
      "2021-10-28  710.000000  713.500000  698.950012  710.489990  710.489990   \n",
      "2021-10-29  713.750000  722.549988  711.909485  717.400024  717.400024   \n",
      "2021-11-01  724.190002  732.369995  718.450012  727.989990  727.989990   \n",
      "2021-11-02  728.390015  742.234985  727.750000  740.440002  740.440002   \n",
      "2021-11-03  739.690002  756.770020  737.109985  755.030029  755.030029   \n",
      "2021-11-04  758.849976  761.109985  739.679993  751.179993  751.179993   \n",
      "2021-11-05  761.479980  762.950012  737.450012  747.520020  747.520020   \n",
      "2021-11-08  750.770020  755.724976  735.059998  742.049988  742.049988   \n",
      "2021-11-09  736.090027  750.940002  734.010010  750.469971  750.469971   \n",
      "2021-11-10  750.000000  754.635010  739.487488  741.919983  741.919983   \n",
      "2021-11-11  746.130005  749.599976  739.125000  741.969971  741.969971   \n",
      "2021-11-12  741.969971  745.369995  728.195007  743.900024  743.900024   \n",
      "2021-11-15  746.080017  756.140015  744.020020  747.989990  747.989990   \n",
      "2021-11-16  746.460022  763.219971  742.406982  754.650024  754.650024   \n",
      "2021-11-17  754.479980  756.400024  735.359985  745.260010  745.260010   \n",
      "2021-11-18  750.000000  754.835022  742.489990  744.969971  744.969971   \n",
      "2021-11-19  739.539978  740.979980  715.419983  727.789978  727.789978   \n",
      "2021-11-22  735.320007  755.000000  735.320007  741.200012  741.200012   \n",
      "2021-11-23  740.080017  745.179993  727.140015  737.309998  737.309998   \n",
      "2021-11-24  732.940002  742.284973  728.200012  740.450012  740.450012   \n",
      "2021-11-26  715.349976  721.767029  700.020020  714.190002  714.190002   \n",
      "2021-11-29  724.859985  733.250000  716.960022  724.500000  724.500000   \n",
      "2021-11-30  719.489990  720.914978  687.770020  692.330017  692.330017   \n",
      "2021-12-01  702.489990  723.349976  675.070007  675.590027  675.590027   \n",
      "2021-12-02  684.469971  716.005005  680.570007  712.200012  712.200012   \n",
      "2021-12-03  713.789978  719.219971  661.640015  674.570007  674.570007   \n",
      "2021-12-06  688.940002  704.645020  682.325012  693.390015  693.390015   \n",
      "2021-12-07  701.489990  720.419983  700.859985  716.780029  716.780029   \n",
      "2021-12-08  724.000000  726.830017  705.020020  710.210022  710.210022   \n",
      "2021-12-09  706.000000  714.320007  703.390015  706.059998  706.059998   \n",
      "2021-12-10  711.099976  717.375000  692.780029  703.919983  703.919983   \n",
      "2021-12-13  702.299988  703.049988  674.400024  675.599976  675.599976   \n",
      "2021-12-14  675.080017  689.219971  667.575012  676.679993  676.679993   \n",
      "2021-12-15  683.000000  703.179993  674.984985  699.530029  699.530029   \n",
      "2021-12-16  706.010010  712.750000  676.828979  682.289978  682.289978   \n",
      "2021-12-17  681.929993  681.929993  647.789978  654.010010  654.010010   \n",
      "2021-12-20  637.950012  640.159973  625.020020  637.640015  637.640015   \n",
      "2021-12-21  650.960022  665.979980  647.460022  665.260010  665.260010   \n",
      "2021-12-22  663.210022  673.924988  661.020020  672.690002  672.690002   \n",
      "2021-12-23  680.000000  682.530029  674.000000  674.760010  674.760010   \n",
      "2021-12-27  674.789978  689.630005  674.789978  689.619995  689.619995   \n",
      "2021-12-28  687.880005  689.940002  677.210022  681.070007  681.070007   \n",
      "2021-12-29  684.619995  688.559998  678.650024  683.190002  683.190002   \n",
      "2021-12-30  681.659973  691.659973  678.650024  679.450012  679.450012   \n",
      "2021-12-31  674.169983  681.260010  671.000000  678.239990  678.239990   \n",
      "2022-01-03  683.700012  692.619995  678.250000  688.169983  688.169983   \n",
      "2022-01-04  701.950012  715.330017  699.479980  706.150024  706.150024   \n",
      "2022-01-05  710.169983  710.169983  678.489990  681.929993  681.929993   \n",
      "2022-01-06  687.200012  734.539978  682.890015  733.289978  733.289978   \n",
      "2022-01-07  733.289978  746.700012  724.980103  733.150024  733.150024   \n",
      "2022-01-10  731.849976  735.369995  693.530029  711.210022  711.210022   \n",
      "2022-01-11  718.070007  733.450012  700.520020  733.450012  733.450012   \n",
      "2022-01-12  737.979980  752.679993  735.844971  737.770020  737.770020   \n",
      "2022-01-13  740.630005  743.969971  705.599976  711.640015  711.640015   \n",
      "2022-01-14  703.500000  705.900024  685.440002  704.409973  704.409973   \n",
      "2022-01-18  699.280029  704.989990  664.650024  668.359985  668.359985   \n",
      "2022-01-19  677.030029  677.250000  646.510010  655.000000  655.000000   \n",
      "2022-01-20  654.270020  674.349976  645.020020  647.940002  647.940002   \n",
      "2022-01-21  628.260010  641.619385  581.760010  581.760010  581.760010   \n",
      "2022-01-24  557.599976  572.700012  530.645691  570.010010  570.010010   \n",
      "2022-01-25  560.450012  566.999878  542.000000  554.820007  554.820007   \n",
      "2022-01-26  572.429993  583.979980  545.780029  560.450012  560.450012   \n",
      "2022-01-27  574.880005  584.780029  538.000000  542.739990  542.739990   \n",
      "2022-01-28  541.130005  560.700012  532.000000  558.469971  558.469971   \n",
      "2022-01-31  558.500000  585.489990  551.880005  583.900024  583.900024   \n",
      "2022-02-01  580.500000  608.000000  577.590027  605.880005  605.880005   \n",
      "2022-02-02  616.000000  617.849976  597.989990  607.989990  607.989990   \n",
      "2022-02-03  599.580017  612.000000  590.010010  601.010010  601.010010   \n",
      "2022-02-04  602.119995  619.794983  597.970520  613.309998  613.309998   \n",
      "2022-02-07  611.549988  629.299988  607.159973  618.400024  618.400024   \n",
      "2022-02-08  624.119995  641.169983  621.489990  635.260010  635.260010   \n",
      "2022-02-09  643.000000  654.900024  635.419983  639.450012  639.450012   \n",
      "2022-02-10  632.000000  657.669983  631.580017  642.309998  642.309998   \n",
      "2022-02-11  632.179993  650.950012  619.380005  627.250000  627.250000   \n",
      "2022-02-14  628.299988  639.114990  615.190002  621.530029  621.530029   \n",
      "2022-02-15  640.719971  648.400024  633.005005  645.229980  645.229980   \n",
      "2022-02-16  637.559998  658.469482  637.559998  654.140015  654.140015   \n",
      "2022-02-17  647.460022  647.900024  618.370789  620.799988  620.799988   \n",
      "2022-02-18  621.390015  627.630005  607.750000  613.130005  613.130005   \n",
      "2022-02-22  607.929993  623.010010  599.090088  607.780029  607.780029   \n",
      "2022-02-23  614.349976  616.890015  584.289978  585.280029  585.280029   \n",
      "2022-02-24  551.859985  589.299988  543.960022  584.270020  584.270020   \n",
      "2022-02-25  593.750000  625.880005  589.710022  623.159973  623.159973   \n",
      "2022-02-28  595.080017  620.039978  595.051025  606.000000  606.000000   \n",
      "2022-03-01  603.580017  606.059998  549.010010  557.880005  557.880005   \n",
      "2022-03-02  567.340027  599.565002  565.880005  594.510010  594.510010   \n",
      "2022-03-03  597.640015  601.919983  580.010010  584.229980  584.229980   \n",
      "2022-03-04  566.900024  574.027527  532.929993  543.119995  543.119995   \n",
      "2022-03-07  542.549988  548.950012  492.820007  494.350006  494.350006   \n",
      "2022-03-08  497.790009  537.830017  495.000000  516.989990  516.989990   \n",
      "2022-03-09  534.229980  549.820007  530.979980  538.940002  538.940002   \n",
      "2022-03-10  534.489990  547.159973  527.270020  539.510010  539.510010   \n",
      "2022-03-11  549.020020  553.185120  529.719971  530.590027  530.590027   \n",
      "2022-03-14  540.849976  542.710022  510.265015  512.119995  512.119995   \n",
      "2022-03-15  515.890015  528.630005  512.167480  526.659973  526.659973   \n",
      "2022-03-16  540.809998  563.239990  534.690002  556.799988  556.799988   \n",
      "2022-03-17  548.010010  566.710022  542.229980  564.729980  564.729980   \n",
      "2022-03-18  564.619995  573.419983  553.000000  570.440002  570.440002   \n",
      "2022-03-21  575.000000  581.674988  556.284485  565.510010  565.510010   \n",
      "2022-03-22  574.099976  595.469971  574.099976  587.859985  587.859985   \n",
      "2022-03-23  579.690002  583.719971  568.299988  569.239990  569.239990   \n",
      "2022-03-24  577.989990  584.257385  568.559998  576.909973  576.909973   \n",
      "2022-03-25  577.780029  590.570007  577.780029  586.400024  586.400024   \n",
      "2022-03-28  583.299988  585.880005  558.465027  576.150024  576.150024   \n",
      "2022-03-29  587.150024  597.159973  578.030029  592.559998  592.559998   \n",
      "2022-03-30  592.369995  592.617493  562.875000  567.039978  567.039978   \n",
      "2022-03-31  566.219971  571.229980  558.684998  559.450012  559.450012   \n",
      "2022-04-01  562.039978  568.659973  537.984985  547.750000  547.750000   \n",
      "2022-04-04  549.169983  553.530029  533.840027  546.900024  546.900024   \n",
      "2022-04-05  545.229980  550.890015  528.260010  529.760010  529.760010   \n",
      "2022-04-06  519.890015  525.200012  508.230011  510.570007  510.570007   \n",
      "2022-04-07  510.570007  515.859985  486.739990  500.279999  500.279999   \n",
      "2022-04-08  500.470001  511.899994  492.885010  500.369995  500.369995   \n",
      "2022-04-11  498.209991  518.684998  490.882812  516.030029  516.030029   \n",
      "2022-04-12  517.429993  529.469971  504.079987  506.839996  506.839996   \n",
      "2022-04-13  500.450012  521.140015  498.500000  520.669983  520.669983   \n",
      "2022-04-14  516.450012  522.390015  506.000000  507.059998  507.059998   \n",
      "2022-04-18  507.089996  507.230011  493.779999  503.700012  503.700012   \n",
      "2022-04-19  510.000000  540.559998  507.704987  537.080017  537.080017   \n",
      "2022-04-20  534.799988  536.760010  516.090027  518.070007  518.070007   \n",
      "2022-04-21  532.299988  532.929993  496.059998  503.130005  503.130005   \n",
      "2022-04-22  573.950012  578.630005  540.969971  541.039978  541.039978   \n",
      "2022-04-25  534.229980  544.729980  520.145020  543.309998  543.309998   \n",
      "2022-04-26  532.840027  539.419983  514.130005  514.510010  514.510010   \n",
      "2022-04-27  517.429993  521.349976  504.980011  508.079987  508.079987   \n",
      "2022-04-28  515.390015  517.739990  499.440002  515.320007  515.320007   \n",
      "2022-04-29  512.679993  519.969971  486.190002  487.640015  487.640015   \n",
      "2022-05-02  491.390015  500.565002  479.100006  497.670013  497.670013   \n",
      "2022-05-03  505.559998  519.400024  498.100006  512.619995  512.619995   \n",
      "2022-05-04  516.429993  540.054993  505.549988  537.510010  537.510010   \n",
      "2022-05-05  524.789978  527.020020  500.670105  513.369995  513.369995   \n",
      "2022-05-06  511.440002  511.440002  466.559998  493.739990  493.739990   \n",
      "2022-05-09  485.269989  489.709991  465.273590  469.809998  469.809998   \n",
      "2022-05-10  474.670013  480.700012  448.739990  459.980011  459.980011   \n",
      "2022-05-11  459.049988  471.850006  439.720001  441.359985  441.359985   \n",
      "2022-05-12  436.299988  448.709991  422.739990  435.570007  435.570007   \n",
      "2022-05-13  448.375000  461.640015  441.929993  449.279999  449.279999   \n",
      "2022-05-16  444.290009  446.519989  430.380005  432.149994  432.149994   \n",
      "2022-05-17  441.000000  451.760010  434.440002  447.989990  447.989990   \n",
      "2022-05-18  439.839996  448.579987  430.130005  431.250000  431.250000   \n",
      "2022-05-19  427.089996  445.029999  425.149994  435.500000  435.500000   \n",
      "2022-05-20  442.649994  444.589996  419.600006  434.700012  434.700012   \n",
      "2022-05-23  442.709991  451.440002  431.690002  448.000000  448.000000   \n",
      "2022-05-24  441.059998  447.065002  420.149994  427.980011  427.980011   \n",
      "2022-05-25  429.140015  457.795013  427.929993  452.720001  452.720001   \n",
      "2022-05-26  455.239990  475.309998  455.089996  473.109985  473.109985   \n",
      "2022-05-27  473.109985  492.459991  473.109985  492.019989  492.019989   \n",
      "2022-05-31  483.829987  494.880005  478.285004  488.570007  488.570007   \n",
      "2022-06-01  492.470001  496.679993  465.839996  474.720001  474.720001   \n",
      "2022-06-02  478.000000  492.149994  472.510010  491.989990  491.989990   \n",
      "2022-06-03  481.970001  484.929993  474.559998  480.279999  480.279999   \n",
      "2022-06-06  492.609985  496.829987  481.070007  483.260010  483.260010   \n",
      "2022-06-07  475.600006  492.480011  472.930206  491.959991  491.959991   \n",
      "2022-06-08  486.750000  491.510010  478.299988  485.859985  485.859985   \n",
      "2022-06-09  486.359985  486.589996  469.640015  470.670013  470.670013   \n",
      "2022-06-10  454.899994  462.890015  440.720001  442.880005  442.880005   \n",
      "2022-06-13  419.519989  424.369995  399.609985  402.140015  402.140015   \n",
      "2022-06-14  408.450012  409.941010  400.040009  406.390015  406.390015   \n",
      "2022-06-15  410.600006  430.549988  406.279999  419.040009  419.040009   \n",
      "2022-06-16  406.690002  406.690002  384.829987  389.130005  389.130005   \n",
      "2022-06-17  398.100006  405.329987  387.989990  399.760010  399.760010   \n",
      "2022-06-21  412.390015  416.489990  401.809998  402.820007  402.820007   \n",
      "2022-06-22  398.769989  405.000000  393.260010  400.149994  400.149994   \n",
      "2022-06-23  399.950012  404.466003  388.464996  401.079987  401.079987   \n",
      "2022-06-24  406.640015  419.339996  405.010010  415.519989  415.519989   \n",
      "2022-06-27  420.709991  424.760010  410.250000  412.279999  412.279999   \n",
      "2022-06-28  416.160004  425.000000  404.339996  404.649994  404.649994   \n",
      "2022-06-29  400.959991  404.450012  393.160004  402.559998  402.559998   \n",
      "2022-06-30  390.250000  399.220001  374.989990  394.989990  394.989990   \n",
      "2022-07-01  393.579987  409.480011  390.934998  400.670013  400.670013   \n",
      "2022-07-05  388.010010  412.489990  385.220001  412.410004  412.410004   \n",
      "2022-07-06  408.779999  417.149994  402.380005  407.230011  407.230011   \n",
      "2022-07-07  413.140015  425.059998  413.140015  423.200012  423.200012   \n",
      "2022-07-08  418.750000  424.850006  412.290009  418.630005  418.630005   \n",
      "2022-07-11  413.130005  417.420013  405.880005  408.510010  408.510010   \n",
      "2022-07-12  405.609985  421.970001  403.549988  411.500000  411.500000   \n",
      "2022-07-13  400.440002  410.095001  395.769989  406.299988  406.299988   \n",
      "2022-07-14  400.209991  400.649994  387.450012  393.140015  393.140015   \n",
      "2022-07-15  402.640015  412.334991  396.040009  408.619995  408.619995   \n",
      "2022-07-18  416.640015  427.230011  411.839996  413.809998  413.809998   \n",
      "2022-07-19  411.000000  427.959991  411.000000  426.700012  426.700012   \n",
      "2022-07-20  427.179993  435.095001  422.105011  434.540009  434.540009   \n",
      "2022-07-21  428.279999  437.859985  421.500000  436.170013  436.170013   \n",
      "2022-07-22  385.000000  392.000000  355.371002  361.359985  361.359985   \n",
      "2022-07-25  366.920013  392.489990  364.809998  391.160004  391.160004   \n",
      "2022-07-26  385.579987  393.570007  364.950012  364.989990  364.989990   \n",
      "2022-07-27  370.459991  383.500000  369.459991  379.429993  379.429993   \n",
      "2022-07-28  378.649994  390.799988  371.760010  390.779999  390.779999   \n",
      "2022-07-29  389.140015  409.640015  387.239990  403.549988  403.549988   \n",
      "2022-08-01  398.549988  401.869995  390.109985  398.000000  398.000000   \n",
      "2022-08-02  395.820007  401.119995  389.619598  400.190002  400.190002   \n",
      "2022-08-03  406.559998  417.670013  406.000000  413.399994  413.399994   \n",
      "2022-08-04  415.760010  417.579987  407.589996  416.260010  416.260010   \n",
      "2022-08-05  414.329987  421.079987  408.960510  413.209991  413.209991   \n",
      "2022-08-08  414.459991  425.700012  412.140015  412.709991  412.709991   \n",
      "2022-08-09  410.730011  420.179993  408.799988  419.769989  419.769989   \n",
      "2022-08-10  432.000000  465.000000  429.515015  452.239990  452.239990   \n",
      "2022-08-11  454.209991  459.989990  451.247406  455.209991  455.209991   \n",
      "2022-08-12  457.829987  464.279999  453.600006  464.239990  464.239990   \n",
      "2022-08-15  456.170013  465.600006  455.720093  465.399994  465.399994   \n",
      "2022-08-16  461.309998  478.130005  460.079987  476.450012  476.450012   \n",
      "2022-08-17  466.369995  467.859985  455.795013  459.809998  459.809998   \n",
      "2022-08-18  460.559998  461.890015  453.989990  457.459991  457.459991   \n",
      "2022-08-19  446.959991  447.920013  431.309998  435.390015  435.390015   \n",
      "2022-08-22  423.959991  426.140015  417.355011  420.869995  420.869995   \n",
      "2022-08-23  424.989990  428.720001  419.309998  422.510010  422.510010   \n",
      "2022-08-24  424.630005  433.404999  422.540009  429.500000  429.500000   \n",
      "2022-08-25  429.480011  439.609985  428.140015  438.510010  438.510010   \n",
      "2022-08-26  440.000000  440.809998  410.500000  410.559998  410.559998   \n",
      "2022-08-29  405.899994  412.380005  404.299988  404.809998  404.809998   \n",
      "2022-08-30  407.230011  409.160004  396.320007  408.190002  408.190002   \n",
      "2022-08-31  410.329987  417.250000  405.390015  406.519989  406.519989   \n",
      "2022-09-01  400.220001  402.250000  389.170013  401.940002  401.940002   \n",
      "2022-09-02  406.899994  414.440002  392.684998  395.100006  395.100006   \n",
      "2022-09-06  392.480011  395.619995  377.100006  383.640015  383.640015   \n",
      "2022-09-07  382.380005  399.679993  381.000000  398.529999  398.529999   \n",
      "2022-09-08  392.079987  415.600006  390.237396  413.980011  413.980011   \n",
      "2022-09-09  421.393005  425.920013  420.049988  422.970001  422.970001   \n",
      "2022-09-12  392.390015  416.381409  390.040009  406.149994  406.149994   \n",
      "2022-09-13  395.869995  398.880005  383.619995  384.690002  384.690002   \n",
      "2022-09-14  383.390015  386.704987  370.079987  377.589996  377.589996   \n",
      "2022-09-15  378.850006  392.459991  377.160004  379.029999  379.029999   \n",
      "2022-09-16  371.660004  371.660004  354.739105  362.420013  362.420013   \n",
      "2022-09-19  356.470001  370.070007  356.470001  365.799988  365.799988   \n",
      "2022-09-20  362.000000  368.820007  360.070007  365.170013  365.170013   \n",
      "2022-09-21  367.000000  376.000000  359.519989  359.529999  359.529999   \n",
      "2022-09-22  360.700012  360.730011  342.149994  342.570007  342.570007   \n",
      "2022-09-23  337.410004  341.529999  331.600006  340.829987  340.829987   \n",
      "2022-09-26  339.000000  348.510010  334.230011  335.700012  335.700012   \n",
      "2022-09-27  338.790009  343.529999  328.265015  333.839996  333.839996   \n",
      "2022-09-28  336.209991  345.964996  334.195007  343.720001  343.720001   \n",
      "2022-09-29  335.760010  340.260010  331.959991  337.929993  337.929993   \n",
      "2022-09-30  336.670013  344.750000  332.649994  335.779999  335.779999   \n",
      "2022-10-03  341.369995  356.690002  337.940002  354.890015  354.890015   \n",
      "2022-10-04  361.390015  377.299988  361.390015  376.489990  376.489990   \n",
      "2022-10-05  363.959991  368.920013  359.739990  364.119995  364.119995   \n",
      "2022-10-06  359.510010  366.649994  353.535004  356.570007  356.570007   \n",
      "2022-10-07  348.950012  349.230011  339.019989  341.470001  341.470001   \n",
      "2022-10-10  346.329987  349.049988  335.690002  340.540009  340.540009   \n",
      "2022-10-11  338.589996  341.510010  330.619995  335.790009  335.790009   \n",
      "2022-10-12  335.279999  340.082001  331.000000  335.250000  335.250000   \n",
      "2022-10-13  322.119995  350.559998  316.040009  345.070007  345.070007   \n",
      "2022-10-14  351.720001  354.804993  317.250000  317.769989  317.769989   \n",
      "2022-10-17  328.399994  334.549988  325.959991  333.510010  333.510010   \n",
      "2022-10-18  342.260010  344.470001  326.149994  330.040009  330.040009   \n",
      "2022-10-19  327.220001  331.855011  312.350006  315.369995  315.369995   \n",
      "2022-10-20  314.709991  321.822906  301.880005  302.459991  302.459991   \n",
      "2022-10-21  255.000000  261.510010  228.520004  230.029999  230.029999   \n",
      "2022-10-24  229.039993  237.699997  228.520004  232.009995  232.009995   \n",
      "2022-10-25  230.699997  240.949997  230.699997  239.500000  239.500000   \n",
      "2022-10-26  237.300003  249.600006  235.634995  241.690002  241.690002   \n",
      "2022-10-27  244.149994  244.149994  227.789993  229.380005  229.380005   \n",
      "2022-10-28  230.000000  237.388397  229.270004  234.149994  234.149994   \n",
      "2022-10-31  232.860001  235.210007  227.360001  230.960007  230.960007   \n",
      "2022-11-01  235.000000  235.660004  227.080002  228.979996  228.979996   \n",
      "2022-11-02  228.119995  228.545700  212.639999  213.050003  213.050003   \n",
      "2022-11-03  210.679993  212.770004  207.350006  207.619995  207.619995   \n",
      "2022-11-04  211.669998  217.399994  206.350006  213.199997  213.199997   \n",
      "2022-11-07  216.020004  217.154999  206.190002  212.910004  212.910004   \n",
      "2022-11-08  212.029999  221.824997  205.369995  218.419998  218.419998   \n",
      "2022-11-09  214.759995  214.759995  204.369995  208.300003  208.300003   \n",
      "2022-11-10  222.380005  239.899994  221.830002  230.949997  230.949997   \n",
      "2022-11-11  233.619995  239.539993  230.339996  235.619995  235.619995   \n",
      "2022-11-14  232.360001  234.320007  219.630005  219.759995  219.759995   \n",
      "2022-11-15  227.429993  244.380005  227.429993  239.929993  239.929993   \n",
      "2022-11-16  236.009995  239.610001  230.720001  235.050003  235.050003   \n",
      "2022-11-17  227.970001  229.250000  218.619995  222.619995  222.619995   \n",
      "2022-11-18  228.399994  229.279999  221.860001  223.529999  223.529999   \n",
      "2022-11-21  221.199997  222.505005  210.110001  215.119995  215.119995   \n",
      "2022-11-22  216.789993  224.380005  215.279999  217.270004  217.270004   \n",
      "2022-11-23  217.009995  225.190002  216.130005  222.199997  222.199997   \n",
      "2022-11-25  222.029999  223.000000  219.070007  222.600006  222.600006   \n",
      "2022-11-28  219.899994  230.279999  216.279999  217.520004  217.520004   \n",
      "2022-11-29  219.800003  223.259995  217.074997  219.130005  219.130005   \n",
      "2022-11-30  220.669998  232.399994  214.500000  231.779999  231.779999   \n",
      "2022-12-01  231.690002  235.850006  225.970001  228.169998  228.169998   \n",
      "2022-12-02  224.380005  227.020004  218.080002  224.919998  224.919998   \n",
      "2022-12-05  220.000000  222.580002  208.119995  209.240005  209.240005   \n",
      "2022-12-06  207.960007  208.979996  198.318207  200.259995  200.259995   \n",
      "2022-12-07  198.100006  209.139999  198.100006  208.279999  208.279999   \n",
      "2022-12-08  210.589996  223.289993  208.250000  222.639999  222.639999   \n",
      "2022-12-09  223.500000  225.000000  219.440002  220.270004  220.270004   \n",
      "2022-12-12  221.839996  226.369995  216.100006  223.399994  223.399994   \n",
      "2022-12-13  233.940002  247.500000  229.070801  234.149994  234.149994   \n",
      "2022-12-14  231.759995  237.779999  228.699997  230.570007  230.570007   \n",
      "2022-12-15  224.149994  228.649994  219.775299  220.600006  220.600006   \n",
      "2022-12-16  219.080002  220.370102  209.990005  213.399994  213.399994   \n",
      "2022-12-19  212.910004  212.910004  208.000107  210.389999  210.389999   \n",
      "2022-12-20  210.179993  214.350006  208.470001  212.460007  212.460007   \n",
      "2022-12-21  216.190002  219.800003  215.160004  217.350006  217.350006   \n",
      "2022-12-22  213.649994  215.979996  207.365005  215.800003  215.800003   \n",
      "2022-12-23  215.130005  216.970001  211.634995  216.850006  216.850006   \n",
      "2022-12-27  216.399994  219.309998  210.330002  216.110001  216.110001   \n",
      "2022-12-28  215.610001  217.809998  212.839996  216.449997  216.449997   \n",
      "2022-12-29  219.529999  236.479996  218.669998  234.630005  234.630005   \n",
      "2022-12-30  229.850006  235.000000  226.119995  230.139999  230.139999   \n",
      "2023-01-03  232.169998  235.000000  221.649994  225.220001  225.220001   \n",
      "2023-01-04  230.100006  241.632507  228.710007  240.059998  240.059998   \n",
      "2023-01-05  235.710007  237.389999  222.410004  232.589996  232.589996   \n",
      "2023-01-06  237.119995  248.190002  231.429993  245.789993  245.789993   \n",
      "2023-01-09  247.960007  254.940002  244.529999  249.429993  249.429993   \n",
      "2023-01-10  246.460007  253.464996  245.199997  252.679993  252.679993   \n",
      "2023-01-11  255.309998  257.519989  251.300003  254.990005  254.990005   \n",
      "2023-01-12  259.190002  261.260010  250.479996  253.820007  253.820007   \n",
      "2023-01-13  248.059998  255.649994  243.169998  252.729996  252.729996   \n",
      "2023-01-17  254.080002  261.450012  249.184998  259.989990  259.989990   \n",
      "2023-01-18  258.959991  265.285004  251.550003  251.979996  251.979996   \n",
      "2023-01-19  246.050003  254.899994  242.524994  250.039993  250.039993   \n",
      "2023-01-20  286.660004  305.674988  274.304993  291.440002  291.440002   \n",
      "2023-01-23  294.730011  305.489990  290.109985  298.690002  298.690002   \n",
      "2023-01-24  294.640015  298.954987  288.279999  288.350006  288.350006   \n",
      "2023-01-25  282.899994  295.329987  278.850006  294.760010  294.760010   \n",
      "2023-01-26  300.000000  301.989990  291.779999  295.619995  295.619995   \n",
      "2023-01-27  293.630005  305.119904  292.480011  302.440002  302.440002   \n",
      "2023-01-30  295.920013  302.329987  290.489990  293.959991  293.959991   \n",
      "2023-01-31  293.959991  302.570007  290.480011  302.440002  302.440002   \n",
      "2023-02-01  300.690002  318.540009  298.799988  313.380005  313.380005   \n",
      "2023-02-02  322.970001  348.059998  321.649994  333.500000  333.500000   \n",
      "2023-02-03  320.769989  335.079987  315.730011  323.209991  323.209991   \n",
      "2023-02-06  316.399994  320.970001  313.528900  316.130005  316.130005   \n",
      "2023-02-07  312.890015  324.459991  309.350006  323.350006  323.350006   \n",
      "2023-02-08  322.850006  325.910004  319.040009  320.399994  320.399994   \n",
      "2023-02-09  324.929993  326.149994  313.549988  315.890015  315.890015   \n",
      "2023-02-10  310.579987  316.480011  308.089996  310.029999  310.029999   \n",
      "2023-02-13  308.700012  311.700012  304.461212  311.559998  311.559998   \n",
      "2023-02-14  309.799988  317.500000  303.089996  310.769989  310.769989   \n",
      "2023-02-15  308.959991  318.260010  307.269989  316.750000  316.750000   \n",
      "2023-02-16  308.369995  310.600006  301.820007  301.929993  301.929993   \n",
      "2023-02-17  298.429993  299.959991  286.049988  292.790009  292.790009   \n",
      "2023-02-21  287.250000  290.269989  280.080109  285.709991  285.709991   \n",
      "2023-02-22  285.470001  291.209991  280.799988  287.549988  287.549988   \n",
      "2023-02-23  290.589996  291.299988  282.049988  289.010010  289.010010   \n",
      "2023-02-24  281.000000  284.516205  278.380005  282.920013  282.920013   \n",
      "2023-02-27  287.589996  289.529999  283.959991  285.929993  285.929993   \n",
      "2023-02-28  288.500000  293.660004  286.089996  288.109985  288.109985   \n",
      "2023-03-01  284.640015  286.970001  280.089996  283.029999  283.029999   \n",
      "2023-03-02  276.390015  278.329987  269.424988  277.170013  277.170013   \n",
      "2023-03-03  280.339996  285.500000  277.859985  284.410004  284.410004   \n",
      "2023-03-06  284.829987  286.519989  280.640015  283.040009  283.040009   \n",
      "2023-03-07  280.390015  283.079987  267.070007  267.390015  267.390015   \n",
      "2023-03-08  266.859985  271.010010  264.000000  267.829987  267.829987   \n",
      "2023-03-09  176.550003  177.749893  100.000000  106.040001  106.040001   \n",
      "2023-03-10  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-13  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-14  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-15  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-16  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-17  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-20  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-21  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-22  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-23  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-24  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-27  106.040001  106.040001  106.040001  106.040001  106.040001   \n",
      "2023-03-29    0.390000    1.290000    0.331000    0.970000    0.970000   \n",
      "2023-03-30    1.350000    1.450000    0.800000    0.895000    0.895000   \n",
      "2023-03-31    0.910000    1.200000    0.700000    0.905000    0.905000   \n",
      "2023-04-03    0.877000    1.170000    0.877000    0.985050    0.985050   \n",
      "2023-04-04    0.990000    1.080000    0.912000    0.970000    0.970000   \n",
      "2023-04-05    0.968000    1.040000    0.881000    0.910100    0.910100   \n",
      "2023-04-06    0.929000    0.929000    0.750000    0.799000    0.799000   \n",
      "2023-04-10    0.807600    0.849000    0.500000    0.605000    0.605000   \n",
      "2023-04-11    0.620000    0.639900    0.524000    0.581000    0.581000   \n",
      "2023-04-12    0.600000    0.640000    0.552000    0.565000    0.565000   \n",
      "2023-04-13    0.565000    0.619000    0.532000    0.590000    0.590000   \n",
      "2023-04-14    0.590000    0.590000    0.540000    0.579000    0.579000   \n",
      "2023-04-17    0.585000    0.585000    0.546000    0.557980    0.557980   \n",
      "2023-04-18    0.533400    0.565000    0.531800    0.550100    0.550100   \n",
      "2023-04-19    0.560000    0.938000    0.550000    0.931000    0.931000   \n",
      "2023-04-20    0.755000    0.840000    0.707000    0.770000    0.770000   \n",
      "2023-04-21    0.665000    0.760600    0.665000    0.711400    0.711400   \n",
      "2023-04-24    0.711100    0.820000    0.711100    0.720000    0.720000   \n",
      "2023-04-25    0.720000    0.725000    0.556500    0.576000    0.576000   \n",
      "2023-04-26    0.600000    0.605000    0.530000    0.590000    0.590000   \n",
      "2023-04-27    0.594000    0.654000    0.561000    0.561000    0.561000   \n",
      "2023-04-28    0.562000    0.600000    0.461000    0.490000    0.490000   \n",
      "2023-05-01    0.471000    0.530000    0.422000    0.510000    0.510000   \n",
      "2023-05-02    0.522550    0.540000    0.452000    0.485000    0.485000   \n",
      "2023-05-03    0.480000    0.500000    0.431000    0.460000    0.460000   \n",
      "2023-05-04    0.461000    0.495900    0.357900    0.412400    0.412400   \n",
      "2023-05-05    0.413000    0.489000    0.413000    0.457500    0.457500   \n",
      "2023-05-08    0.464500    0.500000    0.440000    0.476000    0.476000   \n",
      "2023-05-09    0.476000    0.555000    0.476000    0.519000    0.519000   \n",
      "2023-05-10    0.520000    0.548500    0.455000    0.507500    0.507500   \n",
      "2023-05-11    0.501000    0.522800    0.470000    0.475000    0.475000   \n",
      "2023-05-12    0.476000    0.499000    0.450000    0.485000    0.485000   \n",
      "2023-05-15    0.467000    0.490000    0.467000    0.489000    0.489000   \n",
      "2023-05-16    0.479900    0.489900    0.441000    0.471900    0.471900   \n",
      "2023-05-17    0.477000    0.479900    0.455000    0.465100    0.465100   \n",
      "2023-05-18    0.480000    0.521000    0.470100    0.480000    0.480000   \n",
      "2023-05-19    0.462500    0.490000    0.445000    0.451100    0.451100   \n",
      "2023-05-22    0.470000    0.498500    0.452100    0.456950    0.456950   \n",
      "2023-05-23    0.469900    0.489000    0.442000    0.445000    0.445000   \n",
      "2023-05-24    0.450000    0.450000    0.390100    0.399600    0.399600   \n",
      "2023-05-25    0.438000    0.482000    0.401000    0.410000    0.410000   \n",
      "2023-05-26    0.422000    0.450000    0.400000    0.420000    0.420000   \n",
      "2023-05-30    0.420000    0.430000    0.380000    0.391900    0.391900   \n",
      "2023-05-31    0.390000    0.395000    0.365000    0.385000    0.385000   \n",
      "2023-06-01    0.380000    0.429500    0.357000    0.385000    0.385000   \n",
      "2023-06-02    0.386750    0.390000    0.340500    0.365000    0.365000   \n",
      "2023-06-05    0.360000    0.380000    0.360000    0.361000    0.361000   \n",
      "2023-06-06    0.363900    0.365000    0.340000    0.342000    0.342000   \n",
      "2023-06-07    0.343000    0.362600    0.330000    0.330000    0.330000   \n",
      "2023-06-08    0.330000    0.340000    0.272200    0.300000    0.300000   \n",
      "2023-06-09    0.305000    0.315000    0.300000    0.303780    0.303780   \n",
      "2023-06-12    0.295000    0.318200    0.295000    0.300000    0.300000   \n",
      "2023-06-13    0.300000    0.336000    0.290000    0.310000    0.310000   \n",
      "2023-06-14    0.310000    0.390000    0.310000    0.356500    0.356500   \n",
      "2023-06-15    0.350000    0.580000    0.350000    0.550000    0.550000   \n",
      "2023-06-16    0.575000    0.630000    0.450000    0.475000    0.475000   \n",
      "2023-06-20    0.475000    0.485900    0.411000    0.467500    0.467500   \n",
      "2023-06-21    0.462600    0.465000    0.385000    0.398000    0.398000   \n",
      "2023-06-22    0.409000    0.420000    0.380000    0.381200    0.381200   \n",
      "2023-06-23    0.380000    0.425000    0.380000    0.401200    0.401200   \n",
      "2023-06-26    0.415000    0.477500    0.401000    0.415500    0.415500   \n",
      "2023-06-27    0.410000    0.455000    0.410000    0.447630    0.447630   \n",
      "2023-06-28    0.450000    0.614000    0.450000    0.568000    0.568000   \n",
      "2023-06-29    0.597000    0.650000    0.550000    0.580000    0.580000   \n",
      "2023-06-30    0.581500    0.590000    0.450000    0.517850    0.517850   \n",
      "2023-07-03    0.500000    0.517300    0.480000    0.490000    0.490000   \n",
      "2023-07-05    0.500000    0.580000    0.490000    0.517560    0.517560   \n",
      "2023-07-06    0.512000    0.559000    0.500200    0.511000    0.511000   \n",
      "2023-07-07    0.527000    0.547000    0.519000    0.531500    0.531500   \n",
      "2023-07-10    0.531500    0.540000    0.469880    0.510000    0.510000   \n",
      "2023-07-11    0.502000    0.520000    0.490000    0.518800    0.518800   \n",
      "2023-07-12    0.530000    0.536900    0.480000    0.498000    0.498000   \n",
      "2023-07-13    0.500000    0.580000    0.500000    0.565700    0.565700   \n",
      "2023-07-14    0.580000    0.630000    0.507400    0.560000    0.560000   \n",
      "2023-07-17    0.545000    0.550000    0.480000    0.517500    0.517500   \n",
      "2023-07-18    0.520000    0.522500    0.415000    0.430000    0.430000   \n",
      "2023-07-19    0.225000    0.450000    0.225000    0.450000    0.450000   \n",
      "2023-07-20    0.400000    0.450500    0.400000    0.420000    0.420000   \n",
      "2023-07-21    0.400000    0.425000    0.350000    0.362100    0.362100   \n",
      "2023-07-24    0.366050    0.400000    0.362100    0.382900    0.382900   \n",
      "2023-07-25    0.383000    0.420000    0.383000    0.420000    0.420000   \n",
      "2023-07-26    0.363000    0.400000    0.350000    0.380000    0.380000   \n",
      "2023-07-27    0.380000    0.380000    0.350000    0.361000    0.361000   \n",
      "2023-07-28    0.310000    0.380000    0.310000    0.360000    0.360000   \n",
      "2023-07-31    0.210000    0.360000    0.210000    0.290000    0.290000   \n",
      "2023-08-01    0.220000    0.261000    0.050000    0.170000    0.170000   \n",
      "2023-08-02    0.160000    0.220000    0.140000    0.220000    0.220000   \n",
      "2023-08-03    0.140000    0.210000    0.140000    0.210000    0.210000   \n",
      "2023-08-04    0.140000    0.200000    0.030000    0.160000    0.160000   \n",
      "2023-08-07    0.165000    0.165000    0.110000    0.150000    0.150000   \n",
      "2023-08-08    0.130000    0.140000    0.080000    0.100000    0.100000   \n",
      "2023-08-09    0.100000    0.200000    0.100000    0.100000    0.100000   \n",
      "2023-08-10    0.080000    0.170000    0.080000    0.150000    0.150000   \n",
      "2023-08-11    0.150000    0.240000    0.150000    0.200000    0.200000   \n",
      "2023-08-14    0.230000    0.300000    0.100000    0.100000    0.100000   \n",
      "2023-08-15    0.100000    0.160000    0.100000    0.160000    0.160000   \n",
      "2023-08-16    0.100000    0.160000    0.100000    0.140000    0.140000   \n",
      "2023-08-17    0.120000    0.280000    0.120000    0.120000    0.120000   \n",
      "2023-08-18    0.100000    0.200000    0.080000    0.120000    0.120000   \n",
      "2023-08-21    0.080000    0.150000    0.080000    0.120000    0.120000   \n",
      "2023-08-22    0.080000    0.180000    0.080000    0.120000    0.120000   \n",
      "2023-08-23    0.110000    0.180000    0.110000    0.110000    0.110000   \n",
      "2023-08-24    0.110000    0.140000    0.110000    0.110000    0.110000   \n",
      "2023-08-25    0.080000    0.130000    0.080000    0.110000    0.110000   \n",
      "2023-08-28    0.100000    0.130000    0.080000    0.080000    0.080000   \n",
      "2023-08-29    0.080000    0.120000    0.040000    0.110000    0.110000   \n",
      "2023-08-30    0.051000    0.110000    0.051000    0.110000    0.110000   \n",
      "2023-08-31    0.051000    0.120000    0.051000    0.080000    0.080000   \n",
      "2023-09-01    0.051000    0.110000    0.051000    0.080000    0.080000   \n",
      "2023-09-05    0.051000    0.080000    0.051000    0.051000    0.051000   \n",
      "2023-09-06    0.040000    0.080000    0.040000    0.060000    0.060000   \n",
      "2023-09-07    0.041000    0.070000    0.041000    0.070000    0.070000   \n",
      "2023-09-08    0.041000    0.180000    0.041000    0.070000    0.070000   \n",
      "2023-09-11    0.050000    0.100000    0.041000    0.041000    0.041000   \n",
      "2023-09-12    0.041000    0.100000    0.041000    0.050000    0.050000   \n",
      "2023-09-13    0.060000    0.060000    0.041000    0.050000    0.050000   \n",
      "2023-09-14    0.050000    0.060000    0.045000    0.045000    0.045000   \n",
      "2023-09-15    0.042000    0.050000    0.042000    0.045000    0.045000   \n",
      "2023-09-18    0.045000    0.050000    0.045000    0.050000    0.050000   \n",
      "2023-09-19    0.045000    0.150000    0.045000    0.145000    0.145000   \n",
      "2023-09-20    0.050000    0.100000    0.050000    0.090000    0.090000   \n",
      "2023-09-21    0.090000    0.120000    0.090000    0.090000    0.090000   \n",
      "2023-09-22    0.050000    0.100000    0.050000    0.100000    0.100000   \n",
      "2023-09-25    0.045000    0.110000    0.042000    0.090000    0.090000   \n",
      "2023-09-26    0.090000    0.110000    0.090000    0.090000    0.090000   \n",
      "2023-09-27    0.045000    0.110000    0.045000    0.090000    0.090000   \n",
      "2023-09-28    0.045000    0.100000    0.045000    0.100000    0.100000   \n",
      "2023-09-29    0.090000    0.100000    0.090000    0.090000    0.090000   \n",
      "2023-10-02    0.045000    0.090000    0.045000    0.090000    0.090000   \n",
      "2023-10-03    0.045000    0.110000    0.045000    0.090000    0.090000   \n",
      "2023-10-04    0.045000    0.100000    0.045000    0.090000    0.090000   \n",
      "2023-10-05    0.045000    0.100000    0.045000    0.090000    0.090000   \n",
      "2023-10-06    0.090000    0.100000    0.090000    0.090000    0.090000   \n",
      "2023-10-09    0.045000    0.110000    0.045000    0.090000    0.090000   \n",
      "2023-10-10    0.090000    0.100000    0.090000    0.090000    0.090000   \n",
      "2023-10-11    0.090000    0.100000    0.042000    0.090000    0.090000   \n",
      "2023-10-12    0.090000    0.090000    0.090000    0.090000    0.090000   \n",
      "2023-10-13    0.042000    0.100000    0.042000    0.090000    0.090000   \n",
      "2023-10-16    0.042000    0.110000    0.042000    0.090000    0.090000   \n",
      "2023-10-17    0.042000    0.100000    0.042000    0.090000    0.090000   \n",
      "2023-10-18    0.042000    0.100000    0.042000    0.090000    0.090000   \n",
      "2023-10-19    0.050000    0.110000    0.050000    0.107500    0.107500   \n",
      "2023-10-23    0.090000    0.090000    0.042000    0.042000    0.042000   \n",
      "2023-10-26    0.100000    0.100000    0.040000    0.040000    0.040000   \n",
      "2023-11-01    0.020000    0.060000    0.020000    0.040000    0.040000   \n",
      "2023-11-07    0.050000    0.050000    0.010000    0.012000    0.012000   \n",
      "2023-11-13    0.034000    0.034000    0.010000    0.034000    0.034000   \n",
      "2023-11-16    0.036000    0.040000    0.010000    0.030000    0.030000   \n",
      "2023-11-22    0.030000    0.040000    0.010000    0.010000    0.010000   \n",
      "2023-11-24    0.020000    0.020100    0.010000    0.010000    0.010000   \n",
      "2023-11-29    0.012000    0.030000    0.012000    0.012000    0.012000   \n",
      "\n",
      "              Volume  \n",
      "Date                  \n",
      "2021-10-01    281112  \n",
      "2021-10-04    340741  \n",
      "2021-10-05    358684  \n",
      "2021-10-06    441251  \n",
      "2021-10-07    258661  \n",
      "2021-10-08    316457  \n",
      "2021-10-11    367281  \n",
      "2021-10-12    307513  \n",
      "2021-10-13    243253  \n",
      "2021-10-14    341391  \n",
      "2021-10-15    317824  \n",
      "2021-10-18    329657  \n",
      "2021-10-19    275539  \n",
      "2021-10-20    275657  \n",
      "2021-10-21    334150  \n",
      "2021-10-22    652714  \n",
      "2021-10-25    265132  \n",
      "2021-10-26    232612  \n",
      "2021-10-27    396701  \n",
      "2021-10-28    344331  \n",
      "2021-10-29    407361  \n",
      "2021-11-01    316101  \n",
      "2021-11-02    299218  \n",
      "2021-11-03    310720  \n",
      "2021-11-04    274983  \n",
      "2021-11-05    219176  \n",
      "2021-11-08    211383  \n",
      "2021-11-09    294519  \n",
      "2021-11-10    269891  \n",
      "2021-11-11    137010  \n",
      "2021-11-12    249178  \n",
      "2021-11-15    262502  \n",
      "2021-11-16    312078  \n",
      "2021-11-17    250710  \n",
      "2021-11-18    209436  \n",
      "2021-11-19    470794  \n",
      "2021-11-22    378699  \n",
      "2021-11-23    455876  \n",
      "2021-11-24    233515  \n",
      "2021-11-26    274431  \n",
      "2021-11-29    304757  \n",
      "2021-11-30    748725  \n",
      "2021-12-01    455116  \n",
      "2021-12-02    381371  \n",
      "2021-12-03    514840  \n",
      "2021-12-06    354119  \n",
      "2021-12-07    296907  \n",
      "2021-12-08    316099  \n",
      "2021-12-09    261123  \n",
      "2021-12-10    204364  \n",
      "2021-12-13    356253  \n",
      "2021-12-14    373689  \n",
      "2021-12-15    398189  \n",
      "2021-12-16    377790  \n",
      "2021-12-17    805251  \n",
      "2021-12-20    497925  \n",
      "2021-12-21    305225  \n",
      "2021-12-22    197149  \n",
      "2021-12-23    145381  \n",
      "2021-12-27    126973  \n",
      "2021-12-28    139646  \n",
      "2021-12-29    130647  \n",
      "2021-12-30    143354  \n",
      "2021-12-31    327892  \n",
      "2022-01-03    281558  \n",
      "2022-01-04    403036  \n",
      "2022-01-05    310882  \n",
      "2022-01-06    593157  \n",
      "2022-01-07    404668  \n",
      "2022-01-10    546486  \n",
      "2022-01-11    446637  \n",
      "2022-01-12    351619  \n",
      "2022-01-13    368672  \n",
      "2022-01-14    426532  \n",
      "2022-01-18    536033  \n",
      "2022-01-19    691665  \n",
      "2022-01-20    475957  \n",
      "2022-01-21   1312790  \n",
      "2022-01-24   1056155  \n",
      "2022-01-25    675265  \n",
      "2022-01-26    775409  \n",
      "2022-01-27    475863  \n",
      "2022-01-28    553495  \n",
      "2022-01-31    613096  \n",
      "2022-02-01    674046  \n",
      "2022-02-02    487731  \n",
      "2022-02-03    838779  \n",
      "2022-02-04    526679  \n",
      "2022-02-07    309814  \n",
      "2022-02-08    426087  \n",
      "2022-02-09    432551  \n",
      "2022-02-10    516377  \n",
      "2022-02-11    456812  \n",
      "2022-02-14    485403  \n",
      "2022-02-15    368062  \n",
      "2022-02-16    267119  \n",
      "2022-02-17    333104  \n",
      "2022-02-18    323272  \n",
      "2022-02-22    262237  \n",
      "2022-02-23    337364  \n",
      "2022-02-24    672552  \n",
      "2022-02-25    629755  \n",
      "2022-02-28    527381  \n",
      "2022-03-01    947181  \n",
      "2022-03-02    610520  \n",
      "2022-03-03    372400  \n",
      "2022-03-04    677704  \n",
      "2022-03-07    994679  \n",
      "2022-03-08    842475  \n",
      "2022-03-09    437686  \n",
      "2022-03-10    638632  \n",
      "2022-03-11    613052  \n",
      "2022-03-14    504821  \n",
      "2022-03-15    590502  \n",
      "2022-03-16    659224  \n",
      "2022-03-17    502857  \n",
      "2022-03-18    513382  \n",
      "2022-03-21    496405  \n",
      "2022-03-22    369075  \n",
      "2022-03-23    342950  \n",
      "2022-03-24    369847  \n",
      "2022-03-25    336408  \n",
      "2022-03-28    303921  \n",
      "2022-03-29    324395  \n",
      "2022-03-30    413848  \n",
      "2022-03-31    538033  \n",
      "2022-04-01    467863  \n",
      "2022-04-04    566539  \n",
      "2022-04-05    708070  \n",
      "2022-04-06    929406  \n",
      "2022-04-07    729810  \n",
      "2022-04-08    597071  \n",
      "2022-04-11    716313  \n",
      "2022-04-12    433296  \n",
      "2022-04-13    493959  \n",
      "2022-04-14    713147  \n",
      "2022-04-18    337386  \n",
      "2022-04-19    558196  \n",
      "2022-04-20    629626  \n",
      "2022-04-21    512941  \n",
      "2022-04-22   1147444  \n",
      "2022-04-25    723192  \n",
      "2022-04-26    471377  \n",
      "2022-04-27    437774  \n",
      "2022-04-28    415815  \n",
      "2022-04-29    533259  \n",
      "2022-05-02    544540  \n",
      "2022-05-03    451568  \n",
      "2022-05-04    536515  \n",
      "2022-05-05    558004  \n",
      "2022-05-06    789416  \n",
      "2022-05-09    961999  \n",
      "2022-05-10    805330  \n",
      "2022-05-11    642591  \n",
      "2022-05-12    765464  \n",
      "2022-05-13    467250  \n",
      "2022-05-16    518291  \n",
      "2022-05-17    587344  \n",
      "2022-05-18    712086  \n",
      "2022-05-19    621188  \n",
      "2022-05-20    574276  \n",
      "2022-05-23    463491  \n",
      "2022-05-24    848363  \n",
      "2022-05-25    606839  \n",
      "2022-05-26    489101  \n",
      "2022-05-27    671909  \n",
      "2022-05-31   1178208  \n",
      "2022-06-01    456658  \n",
      "2022-06-02    375813  \n",
      "2022-06-03    367446  \n",
      "2022-06-06    291073  \n",
      "2022-06-07    365538  \n",
      "2022-06-08    529116  \n",
      "2022-06-09    592703  \n",
      "2022-06-10    827728  \n",
      "2022-06-13   1259234  \n",
      "2022-06-14    498222  \n",
      "2022-06-15    699464  \n",
      "2022-06-16    748134  \n",
      "2022-06-17    768818  \n",
      "2022-06-21    468036  \n",
      "2022-06-22    666175  \n",
      "2022-06-23    579031  \n",
      "2022-06-24   1167574  \n",
      "2022-06-27    375702  \n",
      "2022-06-28    318859  \n",
      "2022-06-29    476835  \n",
      "2022-06-30    941894  \n",
      "2022-07-01    389640  \n",
      "2022-07-05    499551  \n",
      "2022-07-06    339257  \n",
      "2022-07-07    364883  \n",
      "2022-07-08    349535  \n",
      "2022-07-11    256020  \n",
      "2022-07-12    290421  \n",
      "2022-07-13    419503  \n",
      "2022-07-14    458302  \n",
      "2022-07-15    594091  \n",
      "2022-07-18    419913  \n",
      "2022-07-19    388227  \n",
      "2022-07-20    377379  \n",
      "2022-07-21    726977  \n",
      "2022-07-22   2255346  \n",
      "2022-07-25   1075307  \n",
      "2022-07-26    764795  \n",
      "2022-07-27    555503  \n",
      "2022-07-28    431644  \n",
      "2022-07-29    655064  \n",
      "2022-08-01    352624  \n",
      "2022-08-02    513025  \n",
      "2022-08-03    528720  \n",
      "2022-08-04    478911  \n",
      "2022-08-05    302509  \n",
      "2022-08-08    352445  \n",
      "2022-08-09    500780  \n",
      "2022-08-10   1202660  \n",
      "2022-08-11    699544  \n",
      "2022-08-12    318026  \n",
      "2022-08-15    254761  \n",
      "2022-08-16    431933  \n",
      "2022-08-17    482324  \n",
      "2022-08-18    275816  \n",
      "2022-08-19    355248  \n",
      "2022-08-22    368905  \n",
      "2022-08-23    361380  \n",
      "2022-08-24    318161  \n",
      "2022-08-25    202500  \n",
      "2022-08-26    434230  \n",
      "2022-08-29    417079  \n",
      "2022-08-30    503896  \n",
      "2022-08-31    418210  \n",
      "2022-09-01    633660  \n",
      "2022-09-02    426472  \n",
      "2022-09-06    441960  \n",
      "2022-09-07    276112  \n",
      "2022-09-08    486375  \n",
      "2022-09-09    441432  \n",
      "2022-09-12   1037960  \n",
      "2022-09-13    795651  \n",
      "2022-09-14    784164  \n",
      "2022-09-15    521152  \n",
      "2022-09-16   1315185  \n",
      "2022-09-19    425696  \n",
      "2022-09-20    428357  \n",
      "2022-09-21    468129  \n",
      "2022-09-22    496042  \n",
      "2022-09-23    607143  \n",
      "2022-09-26    530442  \n",
      "2022-09-27    482567  \n",
      "2022-09-28    488936  \n",
      "2022-09-29    416296  \n",
      "2022-09-30    529279  \n",
      "2022-10-03    538508  \n",
      "2022-10-04    593248  \n",
      "2022-10-05    716129  \n",
      "2022-10-06    450091  \n",
      "2022-10-07    482775  \n",
      "2022-10-10    364102  \n",
      "2022-10-11    430241  \n",
      "2022-10-12    398222  \n",
      "2022-10-13    792284  \n",
      "2022-10-14    943107  \n",
      "2022-10-17    710159  \n",
      "2022-10-18    989674  \n",
      "2022-10-19    668377  \n",
      "2022-10-20   1400445  \n",
      "2022-10-21   5627532  \n",
      "2022-10-24   2449673  \n",
      "2022-10-25   2289904  \n",
      "2022-10-26   1069706  \n",
      "2022-10-27   1758923  \n",
      "2022-10-28   1481346  \n",
      "2022-10-31   1300963  \n",
      "2022-11-01   1253315  \n",
      "2022-11-02   1803147  \n",
      "2022-11-03   1151684  \n",
      "2022-11-04    991671  \n",
      "2022-11-07    786649  \n",
      "2022-11-08    976121  \n",
      "2022-11-09   1021821  \n",
      "2022-11-10   2030919  \n",
      "2022-11-11   1283001  \n",
      "2022-11-14   1025019  \n",
      "2022-11-15   1635044  \n",
      "2022-11-16    880224  \n",
      "2022-11-17   1062673  \n",
      "2022-11-18    918457  \n",
      "2022-11-21   1463004  \n",
      "2022-11-22    692744  \n",
      "2022-11-23    700949  \n",
      "2022-11-25    268465  \n",
      "2022-11-28    835937  \n",
      "2022-11-29    767048  \n",
      "2022-11-30   1462755  \n",
      "2022-12-01    800992  \n",
      "2022-12-02    794695  \n",
      "2022-12-05   1149181  \n",
      "2022-12-06   1747305  \n",
      "2022-12-07   2040076  \n",
      "2022-12-08   1520140  \n",
      "2022-12-09    891727  \n",
      "2022-12-12   1042715  \n",
      "2022-12-13   1973691  \n",
      "2022-12-14    970307  \n",
      "2022-12-15   1050785  \n",
      "2022-12-16   1910394  \n",
      "2022-12-19    725589  \n",
      "2022-12-20    817488  \n",
      "2022-12-21    603757  \n",
      "2022-12-22    814488  \n",
      "2022-12-23    332559  \n",
      "2022-12-27    485233  \n",
      "2022-12-28    480089  \n",
      "2022-12-29   1031536  \n",
      "2022-12-30    710176  \n",
      "2023-01-03    764645  \n",
      "2023-01-04    854687  \n",
      "2023-01-05    985262  \n",
      "2023-01-06   1017532  \n",
      "2023-01-09   1102929  \n",
      "2023-01-10    547669  \n",
      "2023-01-11    745718  \n",
      "2023-01-12    843449  \n",
      "2023-01-13   1100337  \n",
      "2023-01-17    898463  \n",
      "2023-01-18    997496  \n",
      "2023-01-19   1576560  \n",
      "2023-01-20   3090876  \n",
      "2023-01-23   1810326  \n",
      "2023-01-24   1053854  \n",
      "2023-01-25    833787  \n",
      "2023-01-26    666697  \n",
      "2023-01-27    664906  \n",
      "2023-01-30    634982  \n",
      "2023-01-31    738529  \n",
      "2023-02-01   1177025  \n",
      "2023-02-02   2127346  \n",
      "2023-02-03    903745  \n",
      "2023-02-06    607808  \n",
      "2023-02-07    568606  \n",
      "2023-02-08    500533  \n",
      "2023-02-09    746111  \n",
      "2023-02-10    604985  \n",
      "2023-02-13    558686  \n",
      "2023-02-14    641970  \n",
      "2023-02-15    477471  \n",
      "2023-02-16    450039  \n",
      "2023-02-17   1338677  \n",
      "2023-02-21    586270  \n",
      "2023-02-22    855058  \n",
      "2023-02-23    619173  \n",
      "2023-02-24    649155  \n",
      "2023-02-27    583891  \n",
      "2023-02-28    573215  \n",
      "2023-03-01    393543  \n",
      "2023-03-02    746564  \n",
      "2023-03-03    357199  \n",
      "2023-03-06    542443  \n",
      "2023-03-07    830518  \n",
      "2023-03-08    835185  \n",
      "2023-03-09  38746481  \n",
      "2023-03-10         0  \n",
      "2023-03-13         0  \n",
      "2023-03-14         0  \n",
      "2023-03-15         0  \n",
      "2023-03-16         0  \n",
      "2023-03-17         0  \n",
      "2023-03-20         0  \n",
      "2023-03-21         0  \n",
      "2023-03-22         0  \n",
      "2023-03-23         0  \n",
      "2023-03-24         0  \n",
      "2023-03-27         0  \n",
      "2023-03-29  67419705  \n",
      "2023-03-30  55242876  \n",
      "2023-03-31  29616549  \n",
      "2023-04-03  16018917  \n",
      "2023-04-04   6836192  \n",
      "2023-04-05   4847379  \n",
      "2023-04-06   6573026  \n",
      "2023-04-10  13370634  \n",
      "2023-04-11   2875857  \n",
      "2023-04-12   2372428  \n",
      "2023-04-13   1875338  \n",
      "2023-04-14   1730522  \n",
      "2023-04-17   1203277  \n",
      "2023-04-18   1017980  \n",
      "2023-04-19  10192400  \n",
      "2023-04-20   4537117  \n",
      "2023-04-21   1795485  \n",
      "2023-04-24   2424161  \n",
      "2023-04-25   3202509  \n",
      "2023-04-26   1319433  \n",
      "2023-04-27   1112827  \n",
      "2023-04-28   3454430  \n",
      "2023-05-01   1859159  \n",
      "2023-05-02   1106094  \n",
      "2023-05-03   1610293  \n",
      "2023-05-04   3660588  \n",
      "2023-05-05   1440614  \n",
      "2023-05-08    982183  \n",
      "2023-05-09   2142128  \n",
      "2023-05-10   1040369  \n",
      "2023-05-11   1050570  \n",
      "2023-05-12    581771  \n",
      "2023-05-15    409075  \n",
      "2023-05-16   1189701  \n",
      "2023-05-17    409984  \n",
      "2023-05-18    909335  \n",
      "2023-05-19    772012  \n",
      "2023-05-22    737654  \n",
      "2023-05-23    411927  \n",
      "2023-05-24    914176  \n",
      "2023-05-25   1799658  \n",
      "2023-05-26    673901  \n",
      "2023-05-30    595264  \n",
      "2023-05-31    540571  \n",
      "2023-06-01   1106361  \n",
      "2023-06-02   1169996  \n",
      "2023-06-05    214741  \n",
      "2023-06-06    639238  \n",
      "2023-06-07    370760  \n",
      "2023-06-08   1023215  \n",
      "2023-06-09    344937  \n",
      "2023-06-12    423030  \n",
      "2023-06-13    367857  \n",
      "2023-06-14    846266  \n",
      "2023-06-15   1947778  \n",
      "2023-06-16   3294909  \n",
      "2023-06-20    841965  \n",
      "2023-06-21    836134  \n",
      "2023-06-22    344467  \n",
      "2023-06-23    455726  \n",
      "2023-06-26    445717  \n",
      "2023-06-27    530142  \n",
      "2023-06-28   3226809  \n",
      "2023-06-29   3313034  \n",
      "2023-06-30   1194082  \n",
      "2023-07-03    290988  \n",
      "2023-07-05   1075980  \n",
      "2023-07-06    879009  \n",
      "2023-07-07    713888  \n",
      "2023-07-10    818586  \n",
      "2023-07-11    621444  \n",
      "2023-07-12    760297  \n",
      "2023-07-13   2026658  \n",
      "2023-07-14   2512434  \n",
      "2023-07-17   1155408  \n",
      "2023-07-18   2007313  \n",
      "2023-07-19    406001  \n",
      "2023-07-20    342208  \n",
      "2023-07-21    375670  \n",
      "2023-07-24    139354  \n",
      "2023-07-25     93083  \n",
      "2023-07-26    420211  \n",
      "2023-07-27    152103  \n",
      "2023-07-28   1512865  \n",
      "2023-07-31    205033  \n",
      "2023-08-01    304899  \n",
      "2023-08-02     85587  \n",
      "2023-08-03     77949  \n",
      "2023-08-04    115104  \n",
      "2023-08-07     94749  \n",
      "2023-08-08    135155  \n",
      "2023-08-09     96415  \n",
      "2023-08-10     41184  \n",
      "2023-08-11    145275  \n",
      "2023-08-14     99398  \n",
      "2023-08-15     65362  \n",
      "2023-08-16     38706  \n",
      "2023-08-17     18183  \n",
      "2023-08-18     11708  \n",
      "2023-08-21     43569  \n",
      "2023-08-22      8047  \n",
      "2023-08-23     35261  \n",
      "2023-08-24     36015  \n",
      "2023-08-25     26067  \n",
      "2023-08-28     35508  \n",
      "2023-08-29    247638  \n",
      "2023-08-30     30539  \n",
      "2023-08-31     16880  \n",
      "2023-09-01     24155  \n",
      "2023-09-05     52148  \n",
      "2023-09-06     69050  \n",
      "2023-09-07     31120  \n",
      "2023-09-08     19556  \n",
      "2023-09-11     26683  \n",
      "2023-09-12     49943  \n",
      "2023-09-13     12468  \n",
      "2023-09-14     69360  \n",
      "2023-09-15      8776  \n",
      "2023-09-18     22772  \n",
      "2023-09-19     20539  \n",
      "2023-09-20    231390  \n",
      "2023-09-21     18465  \n",
      "2023-09-22    833021  \n",
      "2023-09-25     43821  \n",
      "2023-09-26     26169  \n",
      "2023-09-27    303841  \n",
      "2023-09-28     69949  \n",
      "2023-09-29     23832  \n",
      "2023-10-02      9937  \n",
      "2023-10-03     44586  \n",
      "2023-10-04     34308  \n",
      "2023-10-05     68480  \n",
      "2023-10-06     28211  \n",
      "2023-10-09     58897  \n",
      "2023-10-10     27640  \n",
      "2023-10-11     47795  \n",
      "2023-10-12      6255  \n",
      "2023-10-13     77017  \n",
      "2023-10-16     34896  \n",
      "2023-10-17     32469  \n",
      "2023-10-18     31727  \n",
      "2023-10-19     68291  \n",
      "2023-10-23     12574  \n",
      "2023-10-26     12083  \n",
      "2023-11-01     31011  \n",
      "2023-11-07     21006  \n",
      "2023-11-13     16038  \n",
      "2023-11-16     64817  \n",
      "2023-11-22     17709  \n",
      "2023-11-24     13331  \n",
      "2023-11-29     86311  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ticker = 'SIVBQ'\n",
    "dataset = yf.download(ticker, start=start_date, end=end_date, interval='1d')\n",
    "\n",
    "print(\"num rows\",dataset.shape[0])\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"Num rows for df Close col\",len(dataset['Close'].dropna()))\n",
    "print(dataset)\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.width')\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset\n",
    "\n",
    "Drop row if column is either NaN or missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Gramian angular field (GAF) images from time series data \n",
    "Uses popular time series imaging algorithm in [pyts' GramianAngularField method](https://pyts.readthedocs.io/en/stable/auto_examples/image/plot_single_gaf.html)\n",
    "\n",
    "GAF represents time series in a polar coordinate system instead of the typical Cartesian coordinates by considering the trigonometric sum or difference between stock prices and calculate the correlation within different time intervals. [Source.](https://towardsdatascience.com/rgb-gaf-image-a-possible-solution-to-one-weak-point-of-gramian-angular-field-imaging-ffc6b31edfbe)\n",
    "\n",
    "[Making Gram matrix CNN ready](https://medium.com/analytics-vidhya/encoding-time-series-as-images-b043becbdbf3)\n",
    "\n",
    "The dot product operation in GAF provides us with the similarity between datapoints. Since GAF provides the combination of the datapoints requested, the dot product combination of 32 data points yields 1024 points in an image. Since we request GAF images of size 32x32, we can calculate the number of images GAF will generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaf_images(dataset, gaf_img_sz=32, method=\"summation\", sample_range=(0,1)):\n",
    "    #print(\"len data series received:\",len(dataset),\"size\",dataset.size)\n",
    "\n",
    "    #determine num of gaf_img_szX images with gaf_img_sz datapoints\n",
    "    num_images_to_generate = floor(len(dataset) / gaf_img_sz)\n",
    "    #print(\"num_images_to_generate\",num_images_to_generate)\n",
    "    \n",
    "    #reshape dataset into number of images\n",
    "    dataset = dataset[:num_images_to_generate*gaf_img_sz].reshape(num_images_to_generate, gaf_img_sz)\n",
    "    #print(\"data in GAF\",dataset)\n",
    "    \n",
    "    price_list=[]\n",
    "    for i in range(num_images_to_generate):\n",
    "        price_list.append(np.mean(dataset[i]))\n",
    "    #print(\"prices in GAF\",price_list)\n",
    "    #print(\"image_size\",gaf_img_sz)\n",
    "    \n",
    "    gaf = GramianAngularField(image_size=gaf_img_sz, method=method, sample_range=sample_range)\n",
    "    gaf_images= gaf.fit_transform(dataset)\n",
    "    print(\"gaf_image\",gaf_images.shape)\n",
    "    \n",
    "    return gaf_images, price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Closing Price for one image in GAF\n",
    "A darker patch indicates lower correlation between the different elements of the price time series, possibly due to higher volatility or noise. The opposite is true for the lighter patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524\n",
      "gaf_image (16, 32, 32)\n",
      " (16, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAnnUlEQVR4nO3dfXDV9Zn38U9AcnjKOSEB8iABeVDQQuiUSprVUgoRSO9hoDJ7q+3MQtfB0Q3OKn3MTuvT7k5cO9NqOxT/WBe2M0Wqu0VH74pVLGFrgS5ZWaTW3MCmS1xIUNrkhGBCTH73H97GRkDOJ5zDNwnv18yZgeTKlet3fif55OScXCcriqJIAABcYsNCDwAAuDwRQACAIAggAEAQBBAAIAgCCAAQBAEEAAiCAAIABEEAAQCCuCL0AB/V09OjY8eOKScnR1lZWaHHAQCYoihSW1ubiouLNWzY+e/nDLgAOnbsmEpKSkKPAQC4SI2NjZo0adJ535+xANqwYYO++93vqqmpSXPnztUPf/hDzZ8//4Ifl5OTI0nKV+q/H3zXmGuUUStJ7xm17pU5wqjtMns7c8fM3uPNeuc6H2P2dmbPMXs7nlji1Ue/8Oqd83+311ojjdpxZu9rjNoLf3foa8ZDRvFTZvMGr/xYe+q1xTO83o2HU68d7rW2vjbHxVOvTUZSSduH38/PJyMB9NOf/lTr16/X448/rrKyMj366KNaunSp6uvrNXHixI/92A9+7TZMqQeQ84s690GvTPZ26gfS3O6N3LmRZTLEnVpX3GzuLmB0Aijb7O3Uuz+sON/gxpq9405yujda87f/bUZt3JzF+cHJPczRRm28H4+IXOhhlIw8CeF73/ue1q5dq6985Su67rrr9Pjjj2v06NH6p3/6p0x8OgDAIJT2ADpz5ozq6upUUVHx4ScZNkwVFRXavXv3WfWdnZ1KJpN9LgCAoS/tAfTOO++ou7tbBQUFfd5eUFCgpqams+pramqUSCR6LzwBAQAuD8H/Dqi6ulqtra29l8bGxtAjAQAugbQ/CWH8+PEaPny4mpub+7y9ublZhYWFZ9XHYjHFYu5DmwCAwS7t94Cys7M1b9487dixo/dtPT092rFjh8rLy9P96QAAg1RGnoa9fv16rV69Wp/+9Kc1f/58Pfroo2pvb9dXvvKVTHw6AMAglJEAuuWWW/T222/rvvvuU1NTkz75yU9q+/btZz0xAQBw+cqKosj9m7iMSiaTSiQSGqvU/xas2+jv/qFWJns7v//sMXtncu6EWe/8vaBTK3l/XOpuWXD8W7FX//oxr/6MUft1r7V1HeabvacYtWVm75W3pV4bPen1/i+vXIeM2s+ZvWvNeofzNeGcyzZJsyW1trYqHj//CoXgz4IDAFyeCCAAQBAEEAAgCAIIABAEAQQACIIAAgAEQQABAIIggAAAQRBAAIAgCCAAQBAZ2QWXDqOUejqeNvo6r4EueStw3DR3VqC4q3g6jFp3/Y27jsW5znPM3s7smVzFo0975XOe9+q7jRvAVV5r6zrMM3tfbdTOMXs713nWm17r6eYunjGtqdeOutbrPed3qde6a7Wcr824cfKTkaQ/XriOe0AAgCAIIABAEAQQACAIAggAEAQBBAAIggACAARBAAEAgiCAAABBEEAAgCAIIABAEAQQACCIAbsL7j1JWSnWdht93Z1qZ4zabLN3l1Hrzu1cJ84ckrdnTvJ+ynF3WbnXS8b8wStPmoM756jda23dxp39hZLUZtS2mL2t69xs3m3sdpOkk0Ztodm7xSu3OOc+3mIUR6mVcQ8IABAEAQQACIIAAgAEQQABAIIggAAAQRBAAIAgCCAAQBAEEAAgCAIIABAEAQQACGLAruK5Qqmno7OmxE1cZ72Ou6bEmSWTq3jcuUea9WOM2tFmb2eWHLO3dZ3neb3j5g2x2xjGub6lzF6HCaM21+xtXef5Xuvh5mqlfGe9jnlbyT/m1Tus85lr1EaS/njhMu4BAQCCIIAAAEEQQACAIAggAEAQBBAAIAgCCAAQBAEEAAiCAAIABEEAAQCCIIAAAEEQQACAIAbsLrgRSj0dnRR1955lcs+cs4PLmUMaWLvgnPpM7plzd6RZcjNbP/xM6rU5p7zezvl3d8E59blmb+sD3OZmfcLZBWf2dq5D93vQKOcDnMV+PWIXHABg4Ep7AD3wwAPKysrqc5k1a1a6Pw0AYJDLyK/gPvGJT+jll1/+8JNcMWB/0wcACCQjyXDFFVeosLAwE60BAENERh4DOnTokIqLizVt2jR9+ctf1tGjR89b29nZqWQy2ecCABj60h5AZWVl2rx5s7Zv366NGzeqoaFBn/3sZ9XW1nbO+pqaGiUSid5LSUlJukcCAAxAaQ+gyspK/fmf/7lKS0u1dOlS/fznP1dLS4ueeuqpc9ZXV1ertbW199LY2JjukQAAA1DGnx2Qm5ura665RocPHz7n+2OxmGKxWKbHAAAMMBn/O6BTp07pyJEjKioqyvSnAgAMImkPoK997Wuqra3V73//e/3617/WF7/4RQ0fPly33XZbuj8VAGAQS/uv4N566y3ddtttOnnypCZMmKAbb7xRe/bs0YQJE6w+XUo9HXuMvk5tpns763XcVTzOLM7aHkkytsJIkjqMWvcnouEZ7G0593Ns0lYfGTeAdq+1tf7otNnbqXevwnznA9wn15rDOOWjzN7u+bQY3yhGOSczxb5pD6CtW7emuyUAYAhiFxwAIAgCCAAQBAEEAAiCAAIABEEAAQCCIIAAAEEQQACAIAggAEAQBBAAIAgCCAAQRMZfjqG/3pOUlWKts8vM2Uvm9nZ3qjn17p45d1+bw90H5nCvQ2dHntvbus6bvN5/NJf7OeWtXmvrfDq79yTp7QzVStJVznXe7PU+8wev3pl9onlbMUe3OOd+ijFIlGId94AAAEEQQACAIAggAEAQBBAAIAgCCAAQBAEEAAiCAAIABEEAAQCCIIAAAEEQQACAIAbsKp6YUk9HZ03JSHMOp/cIs7dT766RccTM+hyzfoxRO9rs7ZxPd25LsVc+zj1Q44aYZ675ca7DPK+1Co1a8yqUSjLXPLvdqy98xyg2ZynO4C4e52siy5g7q0cprafiHhAAIAgCCAAQBAEEAAiCAAIABEEAAQCCIIAAAEEQQACAIAggAEAQBBAAIAgCCAAQBAEEAAhiwO6CGy9peIq1Tormm3N0GLXunjmn/ozZ+7RR6+5Ic1Zwuf0zuWcuYfa2lJn1b5v1xn63mbu91s7t0NntJknXGbVXFpjNP2XU1pu9zaV3+W8YxdebvVu8ekvcqJ1i1HZJeuHCZdwDAgAEQQABAIIggAAAQRBAAIAgCCAAQBAEEAAgCAIIABAEAQQACIIAAgAEQQABAIIggAAAQQzYXXCjlPpw7Ubf0eYcTkI7e8kkbweXs5PO5c7t7mvLzVCt5J1Pt3e3UzzBbO7WG7vg3H2Hzu3QXJHmHWYmr0N3z1zSrM/kLO714nB2wTlzpLi8kntAAIAg7ADatWuXli9fruLiYmVlZemZZ57p8/4oinTfffepqKhIo0aNUkVFhQ4dOpSueQEAQ4QdQO3t7Zo7d642bNhwzvc/8sgj+sEPfqDHH39ce/fu1ZgxY7R06VJ1dGTyl0gAgMHGfgyosrJSlZWV53xfFEV69NFH9e1vf1srVqyQJP34xz9WQUGBnnnmGd16660XNy0AYMhI62NADQ0NampqUkVFRe/bEomEysrKtHv3uV8lq7OzU8lkss8FADD0pTWAmpqaJEkFBX2f5lFQUND7vo+qqalRIpHovZSUuK+3CQAYjII/C666ulqtra29l8bGxtAjAQAugbQGUGHh+68Y39zc3Oftzc3Nve/7qFgspng83ucCABj60hpAU6dOVWFhoXbs2NH7tmQyqb1796q8vDydnwoAMMjZz4I7deqUDh8+3Pv/hoYG7d+/X3l5eZo8ebLuuece/d3f/Z2uvvpqTZ06Vd/5zndUXFyslStXpnNuAMAgZwfQvn379PnPf773/+vXr5ckrV69Wps3b9Y3vvENtbe364477lBLS4tuvPFGbd++XSNHOgs/3l8Pk+pwbUZfd43McKPWXfPjXCPuXVVnjYw7dyZX8STM3s4aIWcOm7v/JoOreNx1Oc7t0D1Maxb3OnGaZ3SHkFmf6duKI1OreDpTK7MDaOHChYqi6Lzvz8rK0kMPPaSHHnrIbQ0AuIwEfxYcAODyRAABAIIggAAAQRBAAIAgCCAAQBAEEAAgCAIIABAEAQQACIIAAgAEQQABAIKwV/FcKjFJI1KsTbVO8vZeSVJPBns7e8ycnXSStToso3NL3q65TPZ2d9g5+/Ts5m69cULd6zBm1LpjZ43NYHOn3n2Vl0zWu8eZyVeocWZxalP8psw9IABAEAQQACAIAggAEAQBBAAIggACAARBAAEAgiCAAABBEEAAgCAIIABAEAQQACCIAbuKJ0epr9hxVo+4a0oc7oYNZxb3JwVnjYw7d8Ksz81QreTN7s5tyTPrJ5j1xioedxRnFZPbW/lGrXudXGFMM+EPXu8Wr9ya3T1O50p3d3Y5X0DO3O+mVsY9IABAEAQQACAIAggAEAQBBAAIggACAARBAAEAgiCAAABBEEAAgCAIIABAEAQQACAIAggAEMSA3QU3GPWEHuBPDKRZnL10A6l3RrknaCCdUAxc7hdE4C8g7gEBAIIggAAAQRBAAIAgCCAAQBAEEAAgCAIIABAEAQQACIIAAgAEQQABAIIggAAAQWRFURSFHuJPJZNJJRIJtS6R4iNS/KDXjE/waXOgPxi1eWbvXKO2zezdZNQWm73LzPoJRm2+2TvHqHXPj6P0kPkBdWZ9R+qlR9Z4rUcate5tJevPjOIvmM2/atQeNHv/j1l/2Ki9wez9qlnvGGvUTkq5Mpk8rUTif6u1tVXxePy8ddwDAgAEQQABAIKwA2jXrl1avny5iouLlZWVpWeeeabP+9esWaOsrKw+l2XLlqVrXgDAEGEHUHt7u+bOnasNGzact2bZsmU6fvx47+XJJ5+8qCEBAEOP/XpAlZWVqqys/NiaWCymwsLCfg8FABj6MvIY0M6dOzVx4kTNnDlTd911l06ePHne2s7OTiWTyT4XAMDQl/YAWrZsmX784x9rx44d+od/+AfV1taqsrJS3d3nfum9mpoaJRKJ3ktJSUm6RwIADEBpf0nuW2+9tfffc+bMUWlpqaZPn66dO3dq8eLFZ9VXV1dr/fr1vf9PJpOEEABcBjL+NOxp06Zp/PjxOnz43H+oFYvFFI/H+1wAAENfxgPorbfe0smTJ1VUVJTpTwUAGETsX8GdOnWqz72ZhoYG7d+/X3l5ecrLy9ODDz6oVatWqbCwUEeOHNE3vvENzZgxQ0uXLk3r4ACAwc0OoH379unzn/987/8/ePxm9erV2rhxow4cOKB//ud/VktLi4qLi7VkyRL97d/+rWKxmPV5ol9IqS6pc7Y8zXneGkPJntRr4+79yVyj1twF98eu1GvHjfZ6622z3tkF59RK3i44t7dx7lXq7nb7P2a9sQvuRbN1JnfBzfl16rVXnjKbrzJqa83eR8z6/zRqjXMpSfq5We9IGLUzjNozKVXZAbRw4UJ93P7SF190b/0AgMsRu+AAAEEQQACAIAggAEAQBBAAIAgCCAAQBAEEAAiCAAIABEEAAQCCIIAAAEEQQACAINL+ekDp0vX/L6lIbevQ+7qd/V7GDP3pPdwYPHIGkTe3V5zh+oHU2zqf7n4vt/691Evd43R+DHV7O1+czjHa9Zns7da7vZ3bivstPVNzp1bLPSAAQBAEEAAgCAIIABAEAQQACIIAAgAEQQABAIIggAAAQRBAAIAgCCAAQBAEEAAgiKwoiqLQQ/ypZDKpRCKhWyVlp/gxjUb/q8x52o3aMWbvnAzNIUmtRm2e2XumWZ9v1LqzONe529vxycPmB7xo1hsrcP7vPV7rkUbtlV5rDb/WKL7JbP7YtNRrX/0vr/chr1z1Rm2F2ftls97hfBO6KvXS5GkpsVZqbW1VPB4/bx33gAAAQRBAAIAgCCAAQBAEEAAgCAIIABAEAQQACIIAAgAEQQABAIIggAAAQRBAAIAgCCAAQBBXhB7gfEYq9V1wI8y+jjMZ7J3JuU9nsHcm693esQz2tmT6SjR+VMzkKMOdG63kLetzTqYkaWzqpe6V4i52zOSNPJM3XKe3c366UyvjHhAAIAgCCAAQBAEEAAiCAAIABEEAAQCCIIAAAEEQQACAIAggAEAQBBAAIAgCCAAQxIBdxTNOqW9+aDP65plzOJtHcszeTr2zWkeShhu17nVSaNY7/fPN3s516B6npTjD9V2pl15ptrbW60wymzvHWWL21hRjjgNe6w6v3PomlMnbivOFL0mjjVrnhtWeWhn3gAAAQVgBVFNTo+uvv145OTmaOHGiVq5cqfr6+j41HR0dqqqqUn5+vsaOHatVq1apubk5rUMDAAY/K4Bqa2tVVVWlPXv26KWXXlJXV5eWLFmi9vYP72/de++9eu655/T000+rtrZWx44d080335z2wQEAg5v1GND27dv7/H/z5s2aOHGi6urqtGDBArW2tuqJJ57Qli1btGjRIknSpk2bdO2112rPnj36zGc+k77JAQCD2kU9BtTa2ipJyst7/+Hduro6dXV1qaKiordm1qxZmjx5snbv3n3OHp2dnUomk30uAIChr98B1NPTo3vuuUc33HCDZs+eLUlqampSdna2cnNz+9QWFBSoqanpnH1qamqUSCR6LyUl9lNhAACDUL8DqKqqSgcPHtTWrVsvaoDq6mq1trb2XhobGy+qHwBgcOjX3wGtW7dOzz//vHbt2qVJkz78w4DCwkKdOXNGLS0tfe4FNTc3q7Dw3H89EovFFIvZr8ULABjkrHtAURRp3bp12rZtm1555RVNnTq1z/vnzZunESNGaMeOHb1vq6+v19GjR1VeXp6eiQEAQ4J1D6iqqkpbtmzRs88+q5ycnN7HdRKJhEaNGqVEIqHbb79d69evV15enuLxuO6++26Vl5fzDDgAQB9WAG3cuFGStHDhwj5v37Rpk9asWSNJ+v73v69hw4Zp1apV6uzs1NKlS/WjH/0oLcMCAIaOrCiKotBD/KlkMqlEIqGNkkal+DFvGv1nmfM4K54SZu9M7oJ726h1d7vNNesnGLXuvrassUaxu2jO8fs/8+r/59de/Rmj9n95rTXGqHX3mDk3lkVm74UPGMXmD8F/POHVHzJq58e93r/J4J+mON+EjCcoJ5NS4sr3/1QnHj//8bILDgAQBAEEAAiCAAIABEEAAQCCIIAAAEEQQACAIAggAEAQBBAAIAgCCAAQBAEEAAiiXy/HcCnMl5TqlhVnBc4cc44WozbX7O3UOyuBJG8Vj7td5coC8wOcXTxOreStEnF7W77glV95yuz/XuqlN73htXZeDcV9vUjnC+5zZm/daNT+j9d6XL1XP3+fUbzC7P2vXr0l16idmXppz3uS/u2CZdwDAgAEQQABAIIggAAAQRBAAIAgCCAAQBAEEAAgCAIIABAEAQQACIIAAgAEQQABAIIggAAAQWRFURSFHuJPJZNJJRIJtT4kxUem+EGvGZ/g0+ZAfzBq88zeuUatuwyuyah193t9yqx3drC516GzC+4Kt7nD3DWm35v1xi44d9dYylsXJWmK2XueUevsdpOkxUZti9n7HbPe+YKbZfZ+06x3pPpNVpLGp1yZTLYpkShVa2ur4vH4eeu4BwQACIIAAgAEQQABAIIggAAAQRBAAIAgCCAAQBAEEAAgCAIIABAEAQQACIIAAgAEcUXoAc7rKUnDUyuN/jP1tlnuVosWozbf7J1r1CbN3s1GbbHZu96sLzBq3W0559/ycbYJzl4l000HzQ+oNeuNVTyv/pfX2tnGUnzA61201yh21xk5a36cOSR/VZLzReGsEJKkHWa9I2HUTjJq302pintAAIAgCCAAQBAEEAAgCAIIABAEAQQACIIAAgAEQQABAIIggAAAQRBAAIAgCCAAQBAEEAAgiIG7C65BUlZqpc7mq+nmmqzu1tRrh7urxnKN2jav9Rljlux2r7e9r83ZYzfB7O3sgmsxeztucveYHTHrjV1wh8zWY4zaDrP3yBOp145zlwy+Y9T+3uztnh9nyeT0DPZ25Rq1nWmv5R4QACAIK4Bqamp0/fXXKycnRxMnTtTKlStVX9/3p5aFCxcqKyurz+XOO+9M69AAgMHPCqDa2lpVVVVpz549eumll9TV1aUlS5aovb3v73DWrl2r48eP914eeeSRtA4NABj8rMeAtm/f3uf/mzdv1sSJE1VXV6cFCxb0vn306NEqLCxMz4QAgCHpoh4Dam19/xH6vLy+j0r/5Cc/0fjx4zV79mxVV1fr9OnT5+3R2dmpZDLZ5wIAGPr6/Sy4np4e3XPPPbrhhhs0e/bs3rd/6Utf0pQpU1RcXKwDBw7om9/8purr6/Wzn/3snH1qamr04IMP9ncMAMAg1e8Aqqqq0sGDB/WrX/2qz9vvuOOO3n/PmTNHRUVFWrx4sY4cOaLp089++mF1dbXWr1/f+/9kMqmSkpL+jgUAGCT6FUDr1q3T888/r127dmnSpI9/nfCysjJJ0uHDh88ZQLFYTLFYrD9jAAAGMSuAoijS3XffrW3btmnnzp2aOnXqBT9m//79kqSioqJ+DQgAGJqsAKqqqtKWLVv07LPPKicnR01NTZKkRCKhUaNG6ciRI9qyZYu+8IUvKD8/XwcOHNC9996rBQsWqLS0NCMHAAAYnKwA2rhxo6T3/9j0T23atElr1qxRdna2Xn75ZT366KNqb29XSUmJVq1apW9/+9tpGxgAMDTYv4L7OCUlJaqtrb2ogT5wrD319WfO6qsxxm43STpp1OabvRNGvbkKTm8btYXOSi1J+W949dZ+t0zugnN7Ww6b9f9p1hu74NyVaiONWveGOMKonb/PbN5k1LpXirt/rc6ovSqDvV25Ru0po7YrpSp2wQEAgiCAAABBEEAAgCAIIABAEAQQACAIAggAEAQBBAAIggACAARBAAEAgiCAAABB9Pv1gDKteIYUH55abZ6xZWPUtd4chc56nbwLl/SRm3rpKHMFykRnS0mx11vXm/UFRm2+2TvHqM3oKp4bzPoOs95YxVPxG6+1s4rHva1MdXYlrTCbzzJqF5u9z37pmI93lVHrzpJJY43aKUbtu5K2X7CKe0AAgCAIIABAEAQQACAIAggAEAQBBAAIggACAARBAAEAgiCAAABBEEAAgCAIIABAEAQQACCIAbsLrvFw6mu+3jD6zvmdN0eLUZt/zOvtrDFr91qr2agtdool5bd49dYONndfm7NqzN3V51j+qvkBPzfrjd1xL5utM7kLbm4y9dr5/2o2v8Oo3WH2ftOsrzPrHe714sg1amcatV0pVXEPCAAQBAEEAAiCAAIABEEAAQCCIIAAAEEQQACAIAggAEAQBBAAIAgCCAAQBAEEAAhiwK7iGf7/L5noO1CQ/peYe/K7MzJFPw2QL9WB9AWEQY/vgQCAIAggAEAQBBAAIAgCCAAQBAEEAAiCAAIABEEAAQCCIIAAAEEQQACAIAggAEAQBBAAIIgBsmDqbKMkjU6xdozRN9WeHzhj1OaYvUc58d/j9T5t1LpzK57Bere3M7x7oNYuuLFm84RZ/17qpe5xjjRq3S8ga5Zcs7kzuHt952aw3r2tOL1dTm+nNrXvnNwDAgAEYQXQxo0bVVpaqng8rng8rvLycr3wwgu97+/o6FBVVZXy8/M1duxYrVq1Ss3NzWkfGgAw+FkBNGnSJD388MOqq6vTvn37tGjRIq1YsUK//e1vJUn33nuvnnvuOT399NOqra3VsWPHdPPNN2dkcADA4GY9BrR8+fI+///7v/97bdy4UXv27NGkSZP0xBNPaMuWLVq0aJEkadOmTbr22mu1Z88efeYzn0nf1ACAQa/fjwF1d3dr69atam9vV3l5uerq6tTV1aWKioremlmzZmny5MnavXv3eft0dnYqmUz2uQAAhj47gF5//XWNHTtWsVhMd955p7Zt26brrrtOTU1Nys7OVm5ubp/6goICNTU1nbdfTU2NEolE76WkpMQ+CADA4GMH0MyZM7V//37t3btXd911l1avXq033nij3wNUV1ertbW199LY2NjvXgCAwcP+O6Ds7GzNmDFDkjRv3jz9+7//ux577DHdcsstOnPmjFpaWvrcC2publZhYeF5+8ViMcViMX9yAMCgdtF/B9TT06POzk7NmzdPI0aM0I4dO3rfV19fr6NHj6q8vPxiPw0AYIix7gFVV1ersrJSkydPVltbm7Zs2aKdO3fqxRdfVCKR0O23367169crLy9P8Xhcd999t8rLy3kGHADgLFYAnThxQn/xF3+h48ePK5FIqLS0VC+++KJuuukmSdL3v/99DRs2TKtWrVJnZ6eWLl2qH/3oR/0abFxcimelVjulNfW+8TxvjniLUZzr9Xa2g4xydutImmL8/W9WsddbU8z6CRmqlbxVL25vyySzfoZZb6ziueolr7XzG/ArvdaynlM002w+3qh1z0+nWX/KqHW/gNzrxZFr1E43ajtSqrIC6IknnvjY948cOVIbNmzQhg0bnLYAgMsQu+AAAEEQQACAIAggAEAQBBAAIAgCCAAQBAEEAAiCAAIABEEAAQCCIIAAAEHY27AzLYoiSVIySv1j2oz+Tl9JklPv9u7JUK28UbLM3uoy688Yte4GlBFG7btmb0fS3JVkXSmStYrHHaXbqG03ezuvL9ljHKMk7yvfPfnuDdH5onBncb/gHM7tMLX1OpKUTL5//X3w/fx8sqILVVxib731Fi9KBwBDQGNjoyZNOv8evgEXQD09PTp27JhycnKUlfXhNtJkMqmSkhI1NjYqHo8HnDCzOM6h43I4RonjHGrScZxRFKmtrU3FxcUaNuz8j/QMuF/BDRs27GMTMx6PD+mT/wGOc+i4HI5R4jiHmos9zkTiwuv+eRICACAIAggAEMSgCaBYLKb7779fsZjz6lmDD8c5dFwOxyhxnEPNpTzOAfckBADA5WHQ3AMCAAwtBBAAIAgCCAAQBAEEAAhi0ATQhg0bdNVVV2nkyJEqKyvTb37zm9AjpdUDDzygrKysPpdZs2aFHuui7Nq1S8uXL1dxcbGysrL0zDPP9Hl/FEW67777VFRUpFGjRqmiokKHDh0KM+xFuNBxrlmz5qxzu2zZsjDD9lNNTY2uv/565eTkaOLEiVq5cqXq6+v71HR0dKiqqkr5+fkaO3asVq1apebm5kAT908qx7lw4cKzzuedd94ZaOL+2bhxo0pLS3v/2LS8vFwvvPBC7/sv1bkcFAH005/+VOvXr9f999+v//iP/9DcuXO1dOlSnThxIvRoafWJT3xCx48f77386le/Cj3SRWlvb9fcuXO1YcOGc77/kUce0Q9+8AM9/vjj2rt3r8aMGaOlS5eqoyP1pYcDwYWOU5KWLVvW59w++eSTl3DCi1dbW6uqqirt2bNHL730krq6urRkyRK1t3+4nfTee+/Vc889p6efflq1tbU6duyYbr755oBT+1I5Tklau3Ztn/P5yCOPBJq4fyZNmqSHH35YdXV12rdvnxYtWqQVK1bot7/9raRLeC6jQWD+/PlRVVVV7/+7u7uj4uLiqKamJuBU6XX//fdHc+fODT1GxkiKtm3b1vv/np6eqLCwMPrud7/b+7aWlpYoFotFTz75ZIAJ0+OjxxlFUbR69epoxYoVQebJlBMnTkSSotra2iiK3j93I0aMiJ5++unemt/97neRpGj37t2hxrxoHz3OKIqiz33uc9Ff//VfhxsqQ8aNGxf94z/+4yU9lwP+HtCZM2dUV1enioqK3rcNGzZMFRUV2r17d8DJ0u/QoUMqLi7WtGnT9OUvf1lHjx4NPVLGNDQ0qKmpqc95TSQSKisrG3LnVZJ27typiRMnaubMmbrrrrt08uTJ0CNdlNbWVklSXl6eJKmurk5dXV19zuesWbM0efLkQX0+P3qcH/jJT36i8ePHa/bs2aqurtbp0+5rYAwc3d3d2rp1q9rb21VeXn5Jz+WAW0b6Ue+88466u7tVUFDQ5+0FBQV68803A02VfmVlZdq8ebNmzpyp48eP68EHH9RnP/tZHTx4UDk5OaHHS7umpiZJOud5/eB9Q8WyZct08803a+rUqTpy5Ij+5m/+RpWVldq9e7eGDx8eejxbT0+P7rnnHt1www2aPXu2pPfPZ3Z2tnJzc/vUDubzea7jlKQvfelLmjJlioqLi3XgwAF985vfVH19vX72s58FnNb3+uuvq7y8XB0dHRo7dqy2bdum6667Tvv3779k53LAB9DlorKysvffpaWlKisr05QpU/TUU0/p9ttvDzgZLtatt97a++85c+aotLRU06dP186dO7V48eKAk/VPVVWVDh48OOgfo7yQ8x3nHXfc0fvvOXPmqKioSIsXL9aRI0c0ffr0Sz1mv82cOVP79+9Xa2ur/uVf/kWrV69WbW3tJZ1hwP8Kbvz48Ro+fPhZz8Bobm5WYWFhoKkyLzc3V9dcc40OHz4cepSM+ODcXW7nVZKmTZum8ePHD8pzu27dOj3//PP65S9/2edlUwoLC3XmzBm1tLT0qR+s5/N8x3kuZWVlkjTozmd2drZmzJihefPmqaamRnPnztVjjz12Sc/lgA+g7OxszZs3Tzt27Oh9W09Pj3bs2KHy8vKAk2XWqVOndOTIERUVFYUeJSOmTp2qwsLCPuc1mUxq7969Q/q8Su+/6u/JkycH1bmNokjr1q3Ttm3b9Morr2jq1Kl93j9v3jyNGDGiz/msr6/X0aNHB9X5vNBxnsv+/fslaVCdz3Pp6elRZ2fnpT2XaX1KQ4Zs3bo1isVi0ebNm6M33ngjuuOOO6Lc3Nyoqakp9Ghp89WvfjXauXNn1NDQEL366qtRRUVFNH78+OjEiROhR+u3tra26LXXXotee+21SFL0ve99L3rttdei//7v/46iKIoefvjhKDc3N3r22WejAwcORCtWrIimTp0avfvuu4En93zccba1tUVf+9rXot27d0cNDQ3Ryy+/HH3qU5+Krr766qijoyP06Cm76667okQiEe3cuTM6fvx47+X06dO9NXfeeWc0efLk6JVXXon27dsXlZeXR+Xl5QGn9l3oOA8fPhw99NBD0b59+6KGhobo2WefjaZNmxYtWLAg8OSeb33rW1FtbW3U0NAQHThwIPrWt74VZWVlRb/4xS+iKLp053JQBFAURdEPf/jDaPLkyVF2dnY0f/78aM+ePaFHSqtbbrklKioqirKzs6Mrr7wyuuWWW6LDhw+HHuui/PKXv4wknXVZvXp1FEXvPxX7O9/5TlRQUBDFYrFo8eLFUX19fdih++HjjvP06dPRkiVLogkTJkQjRoyIpkyZEq1du3bQ/fB0ruOTFG3atKm35t13343+6q/+Kho3blw0evTo6Itf/GJ0/PjxcEP3w4WO8+jRo9GCBQuivLy8KBaLRTNmzIi+/vWvR62trWEHN/3lX/5lNGXKlCg7OzuaMGFCtHjx4t7wiaJLdy55OQYAQBAD/jEgAMDQRAABAIIggAAAQRBAAIAgCCAAQBAEEAAgCAIIABAEAQQACIIAAgAEQQABAIIggAAAQRBAAIAg/h+6stQfPqI36gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gaf_method='summation'\n",
    "\n",
    "feature_data = np.array(dataset['Close'])\n",
    "print(len(dataset['Close']))\n",
    "#print(\"feature_dataset_array\",feature_data)\n",
    "#print(\"len datapoints in dataset\",len(feature_data))\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "gaf_images,price_list= generate_gaf_images(feature_data,gaf_img_sz=32, method=gaf_method)\n",
    "\n",
    "print(\"\",gaf_images.shape)\n",
    "plt.imshow(gaf_images[0],cmap='hot')\n",
    "#print(\"gaf_images.shape\",gaf_images.shape,\"data points in images\",gaf_images.size, \"gaf image [0] shape\",gaf_images[0].shape, \"data points in image [0]\",gaf_images[0].size)\n",
    "#print(\"gasf image[0] data\",gaf_images[0])\n",
    "#print(\"Gasf data\",gasf_images)\n",
    "\n",
    "np.set_printoptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape images array (1, 1, 5, 491, 32, 32) shape image (32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_array=np.array(feature_image_dataset_list)\n",
    "prices_array=np.array(feature_price_dataset_list)\n",
    "labels_array=np.array(feature_label_dataset_list)\n",
    "\n",
    "# Plot the first image of each column\n",
    "fig, axes = plt.subplots(nrows=1, ncols=cols_used_count, figsize=(20, 6))\n",
    "for ax in axes:\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "#EXPLANATION SHAPE\n",
    "#shape images array (1, 1, 5, 491, 32, 32)\n",
    "#I get 16 images (32x32) for 524 data points, i.e. close full price series.\n",
    "#Since I create images for a sliding window of price series, I request ~30 time series into images\n",
    "#30*16=480~491 -  difference due to smaller last window\n",
    "print(\"shape images array\",images_array.shape,\"shape image\",images_array[0][0][0][0].shape)\n",
    "for i in range(cols_used_count):\n",
    "    axes[i].imshow(images_array[0][0][i][0], cmap='hot')\n",
    "    axes[i].set_title(f\"Column {cols_used[i]} \")\n",
    "\n",
    "#average first image of all features\n",
    "#interval_between_features = images_array[0][0][0][0][0].shape\n",
    "average_images = []\n",
    "for i in range(cols_used_count):\n",
    "    average_images.append(images_array[0][0][i][0])\n",
    "    #print(\"this image shape\",images_array[0, image_index].shape)\n",
    "\n",
    "average_image = np.mean(average_images, axis=0)\n",
    "\n",
    "# Hide axes\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Plot the average image separately\n",
    "plt.figure()  # Create a new figure for the average image\n",
    "plt.imshow(average_image, cmap='hot')\n",
    "plt.title(\"Average Image\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate images for open, high, low, close and adj close prices\n",
    "\n",
    "We transpose the resulting image list to represent:\n",
    "+ 16: number of images\n",
    "+ 5: number of image channels/features. Each image has 5 =\"Open\", \"High\", \"Low\", \"Close\", and \"Adj Close\"\n",
    "+ 32: image height\n",
    "+ 32: image width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples 524\n",
      "Target number of chunks to create an image for Feature Open 491\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "num_samples 524\n",
      "Target number of chunks to create an image for Feature High 491\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "num_samples 524\n",
      "Target number of chunks to create an image for Feature Low 491\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "num_samples 524\n",
      "Target number of chunks to create an image for Feature Close 491\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "num_samples 524\n",
      "Target number of chunks to create an image for Feature Adj Close 491\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "gaf_image (1, 32, 32)\n",
      "len images 5 len price list 5 len labels 491\n",
      "Shape of images before transpose: (5, 1, 491, 1, 32, 32)\n",
      "Shape of images after transpose: (1, 1, 5, 491, 32, 32)\n",
      "shape [0] set (1, 5, 491, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "cols_used = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"]\n",
    "cols_used_count = sum(column_name in cols_used for column_name in dataset.columns)\n",
    "\n",
    "def generate_multiple_feature_images(dataset, image_size=32, method=\"summation\", sample_range = (0, 1)):\n",
    "    \n",
    "    feature_image_dataset_list=[[] for _ in range(len(cols_used))]\n",
    "    feature_price_dataset_list=[[] for _ in range(len(cols_used))] #=\"Open\", \"High\", \"Low\", \"Close\" , \"Adj Close\"\n",
    "    feature_label_dataset_list=[] #next value for each chunk of =\"Open\", \"High\", \"Low\", \"Close\" , \"Adj Close\"\n",
    "    \n",
    "    column_idx = 0\n",
    "\n",
    "    for idx, column_name in enumerate(dataset.columns):\n",
    "\n",
    "      #create open, high, low and close images\n",
    "      if column_name in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"]:\n",
    "        temp_image_list = []\n",
    "        temp_price_list = []\n",
    "        #print(\"dataset idx\", idx, \"len rows this data feature\", len(dataset[i]), \"dataset[i].shape\", dataset[i].shape, \"dataset i:\", dataset[i])\n",
    "\n",
    "        feature_data = dataset[column_name].values\n",
    "        num_samples = len(feature_data)\n",
    "        print(\"num_samples\",num_samples)\n",
    "\n",
    "        #print(\"feature_dataset_array idx\", idx, \"len rows feature_dataset_array\", len(feature_dataset_array[0]), \"shape\",feature_dataset_array.shape, \"feature_dataset_array:\", feature_dataset_array)\n",
    "        \n",
    "        # generate_gaf_images generates images from full dataset according to image_size\n",
    "        # however, we loop by data_chunk so each chunk represents the price series that we slide by one day forward\n",
    "        # and generate an image for each slice\n",
    "        # TODO: parallelism\n",
    "        # we add cushion to slide the window forward\n",
    "        print(f\"Target number of chunks to create an image for Feature {column_name}\",num_samples - (image_size+1))\n",
    "        for cur_chunk in range(num_samples - (image_size+1)):\n",
    "          \n",
    "          #chunk size of image size\n",
    "          data_chunk = feature_data[cur_chunk:cur_chunk+image_size]\n",
    "          \n",
    "          #append gaf image to image list. store price feature values in price list\n",
    "          gaf_images, price_list = generate_gaf_images(data_chunk, gaf_img_sz=image_size, method=method, sample_range=sample_range)\n",
    "          temp_image_list.append(gaf_images)\n",
    "          \n",
    "          # if(cur_chunk==0):\n",
    "          #   print(\"Price Data Pre-Gaf: i\", cur_chunk, \"len\",len(data_chunk), \"shape\", feature_data.shape, \"data\",data_chunk)\n",
    "          #   print(\"Image Returned: idx\", idx, \"image size\", gaf_images.size, f\"first {image_size} image vals\", gaf_images.flatten()[:image_size])\n",
    "          \n",
    "          temp_price_list.append(price_list)\n",
    "          \n",
    "          #get next single value after the chunk as label to list\n",
    "          if(column_name == \"Open\"):\n",
    "            feature_label_dataset_list.append(cur_chunk + image_size + 1)\n",
    "        \n",
    "        feature_image_dataset_list[column_idx].append(temp_image_list)\n",
    "        feature_price_dataset_list[column_idx].append(price_list)\n",
    "        column_idx += 1\n",
    "\n",
    "\n",
    "    print(\"len images\",len(feature_image_dataset_list),\"len price list\",len(feature_price_dataset_list), \"len labels\", len(feature_label_dataset_list)) # 2455=total range*5\n",
    "    \n",
    "    feature_image_dataset_list = np.array(feature_image_dataset_list) \n",
    "    print(\"Shape of images before transpose:\", feature_image_dataset_list.shape)\n",
    "    \n",
    "    #transpose image for CNN\n",
    "    #(5, 1, 491, 1, 32, 32)\n",
    "    feature_image_dataset_list= np.transpose(feature_image_dataset_list, (1, 3, 0, 2, 4, 5))\n",
    "    print(\"Shape of images after transpose:\", feature_image_dataset_list.shape)\n",
    "\n",
    "    return feature_image_dataset_list, feature_price_dataset_list, feature_label_dataset_list\n",
    "\n",
    "#Generate images from dataset\n",
    "gaf_method=\"summation\"\n",
    "sample_range = (0, 1)\n",
    "image_size = 32 #(x,y)\n",
    "feature_image_dataset_list, feature_price_dataset_list, feature_label_dataset_list = generate_multiple_feature_images(dataset, image_size=image_size, method=gaf_method, sample_range=sample_range)\n",
    "\n",
    "print(\"shape [0] set\",np.array(feature_image_dataset_list[0]).shape)\n",
    "\n",
    "np.set_printoptions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual representation of feature images for the same time range\n",
    "\n",
    "Bottom image averages depth values of feature images (channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape images array (1, 1, 5, 491, 32, 32) shape image (32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_array=np.array(feature_image_dataset_list)\n",
    "prices_array=np.array(feature_price_dataset_list)\n",
    "labels_array=np.array(feature_label_dataset_list)\n",
    "\n",
    "# Plot the first image of each column\n",
    "fig, axes = plt.subplots(nrows=1, ncols=cols_used_count, figsize=(20, 6))\n",
    "for ax in axes:\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "#EXPLANATION SHAPE\n",
    "#shape images array (1, 1, 5, 491, 32, 32)\n",
    "#I get 16 images (32x32) for 524 data points, i.e. close full price series.\n",
    "#Since I create images for a sliding window of price series, I request ~30 time series into images\n",
    "#30*16=480~491 -  difference due to smaller last window\n",
    "print(\"shape images array\",images_array.shape,\"shape image\",images_array[0][0][0][0].shape)\n",
    "for i in range(cols_used_count):\n",
    "    axes[i].imshow(images_array[0][0][i][0], cmap='hot')\n",
    "    axes[i].set_title(f\"Column {cols_used[i]} \")\n",
    "\n",
    "#average first image of all features\n",
    "#interval_between_features = images_array[0][0][0][0][0].shape\n",
    "average_images = []\n",
    "for i in range(cols_used_count):\n",
    "    average_images.append(images_array[0][0][i][0])\n",
    "    #print(\"this image shape\",images_array[0, image_index].shape)\n",
    "\n",
    "average_image = np.mean(average_images, axis=0)\n",
    "\n",
    "# Hide axes\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Plot the average image separately\n",
    "plt.figure()  # Create a new figure for the average image\n",
    "plt.imshow(average_image, cmap='hot')\n",
    "plt.title(\"Average Image\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training/Testing Datasets for Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Transform to image to convert to tensor and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetTransform(normalize_ftor=0.5,resolution_x=32,resolution_y=32):\n",
    "    return transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([normalize_ftor], [normalize_ftor])\n",
    "    #transforms.Resize((resolution_x, resolution_y))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 5, 491, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "feature_image_dataset_list_f32 = np.array(images_array).astype(np.float32)\n",
    "#images_array = np.transpose(feature_image_dataset_list, (1, 0, 2, 3))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "labels_array = np.array(labels_array)#.astype(np.float32)\n",
    "reshaped_labels_array = labels_array.reshape(-1, 1)\n",
    "labels_scaled_list_f32 = scaler.fit_transform(reshaped_labels_array).reshape(-1,).astype(np.float32)\n",
    "\n",
    "print(images_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features data (close, high, low, etc) for Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.transform = SetTransform()\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.inputs[index]\n",
    "        Y = self.labels[index]\n",
    "        return X, Y\n",
    "  \n",
    "    def prepare_ordered_dataset(self):\n",
    "        x = []\n",
    "        y = []\n",
    "        #print(\"len inputs\", len(self.inputs), self.inputs.shape, self.inputs.shape[0])\n",
    "        for feature in range(self.inputs.shape[0]):\n",
    "            #print(\"feature\",feature,self.inputs[feature].shape[0])\n",
    "            for image in range(self.inputs[feature].shape[0]):\n",
    "                #print(\"feature-image\",feature,image,\"len image\",len(self.inputs[feature][image]))\n",
    "                #print(\"size image\",self.inputs[feature][image].size)\n",
    "                #print(\"label\",self.labels[image])\n",
    "                self.inputs[feature][image] = self.transform(self.inputs[feature][image])\n",
    "                x.append(np.expand_dims(self.inputs[feature][image], axis=0))\n",
    "                y.append(self.labels[image])\n",
    "                #print(self.labels[image])\n",
    "                #print(self.inputs[feature][image].shape)\n",
    "        \n",
    "        #cnn requests labels size (4,1) instead of (4)\n",
    "        y = np.expand_dims(y, axis=1) \n",
    "        #print(\"size self\",self.inputs[0].shape,self.inputs[1].shape)\n",
    "        dataset = [(img, label) for img, label in zip(x, y)]\n",
    "        return dataset\n",
    "        \n",
    "        #return np.array(x),np.array(y)\n",
    "    \n",
    "    def split_data(self,dataset, batch_size=10, test_size=0.2, train_shuffle=False):\n",
    "        num_samples = len(dataset)\n",
    "        #print(\"numsamples\",num_samples)\n",
    "        num_test_samples = int(test_size * num_samples)\n",
    "        num_train_samples = num_samples - num_test_samples\n",
    "        #print(\"num_train_samples\",num_train_samples)\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        train_indices = indices[:num_train_samples]\n",
    "        test_indices = indices[num_train_samples:]\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler,shuffle=train_shuffle)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "        sample_batch = next(iter(train_loader))\n",
    "        input_shape = sample_batch[0].shape\n",
    "        label_shape = sample_batch[1].shape\n",
    "        print(\"input_shape\",input_shape,\"label_shape\",label_shape)\n",
    "\n",
    "        return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 491, 32, 32) 5\n",
      "input_shape torch.Size([10, 1, 32, 32]) label_shape torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "def Generate_Train_And_Test_Loaders(feature_image_dataset_list_f32,labels_scaled_list_f32, train_shuffle=False, batch_size=10):\n",
    "\n",
    "    print(feature_image_dataset_list_f32[0][0].shape, feature_image_dataset_list_f32[0][0].shape[0])\n",
    "\n",
    "    #reshape for cnn\n",
    "    reshaped_feature_image_dataset_list_f32 = np.expand_dims(feature_image_dataset_list_f32[0][0].reshape(-1, *feature_image_dataset_list_f32[0][0].shape[2:]), axis=1)\n",
    "    #print(\"res\",reshaped_feature_image_dataset_list_f32.shape)\n",
    "\n",
    "    dataPreObject = DataPrep(reshaped_feature_image_dataset_list_f32, labels_scaled_list_f32)\n",
    "    #print(\"dataprep image:\",dataPreObject[0][0].shape,\"label\",dataPreObject[0][1].size)\n",
    "\n",
    "    #print(\"feature_image_dataset_list_f32\",feature_image_dataset_list_f32[0][0].shape)\n",
    "    #print(\"labels_scaled_list_f32\",labels_scaled_list_f32.shape)\n",
    "    dataset = dataPreObject.prepare_ordered_dataset()\n",
    "\n",
    "    # for c in range(len(dataset[0])):\n",
    "    #     print(f\"size labels {c}\",dataset[1][c].size)\n",
    "    #     print(f\"size image {c}\",dataset[0][c].shape)\n",
    "\n",
    "    batch_size = batch_size\n",
    "    train_loader, test_loader = dataPreObject.split_data(dataset, \n",
    "                                                         batch_size=batch_size,\n",
    "                                                         train_shuffle=train_shuffle)\n",
    "\n",
    "    # for e in train_loader:\n",
    "    #     print(\"type\",type(e))\n",
    "    #     print(\"imga\",e[0].shape,e[0].size)\n",
    "    #     print(\"label\",e[1].shape,e[1].size)\n",
    "\n",
    "    return train_loader,test_loader\n",
    "    \n",
    "\n",
    "train_loader,test_loader = Generate_Train_And_Test_Loaders(feature_image_dataset_list_f32,labels_scaled_list_f32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dynamically calulate the shape of layers outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_shape_dynamic(h_w, kernel_size=(1,1), stride=1):\n",
    "        h = floor( (h_w[0] - kernel_size[0])/ stride) + 1\n",
    "        w = floor( (h_w[1] - kernel_size[1])/ stride) + 1\n",
    "        return h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,name=\"LeNet5Blend\",\n",
    "                 filter_size_1=(2, 2), filter_size_2=(2, 3), filter_size_3=(2, 3), stride=2,\n",
    "                 output_conv_1=80,output_conv_2=16, output_FC_1=120,output_FC_2=84,\n",
    "                 image_resolution_x=32,image_resolution_y=32,final_FCLayer_outputs=1,\n",
    "                 dropout_probab=0):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        if name:\n",
    "            self.name = name\n",
    "        self.totalparams = 0\n",
    "        self.output_conv_2=output_conv_2\n",
    "        self.conv_output_size=0\n",
    "        print();print(\"Convos & dropoutP:\",output_conv_1,output_conv_2, dropout_probab)\n",
    "        \n",
    "        #num channels input, num channels output, filter size \n",
    "        self.conv1 = nn.Conv2d(1, output_conv_1, filter_size_1,stride=1)\n",
    "        #filtersize,stride.\n",
    "        #maxpool acts the same way in each channel, so doesn't need to be fed the num channels of the input\n",
    "        self.pool = nn.MaxPool2d(kernel_size=filter_size_2, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(output_conv_1, output_conv_2, filter_size_3,stride=1)\n",
    "\n",
    "        H_out_1, W_out_1 = conv_output_shape_dynamic((image_resolution_y, image_resolution_x), kernel_size=filter_size_1,stride=1)\n",
    "        H_out_2, W_out_2 = conv_output_shape_dynamic((H_out_1, W_out_1), kernel_size=filter_size_2,stride=stride)\n",
    "        H_out_3, W_out_3 = conv_output_shape_dynamic((H_out_2, W_out_2), kernel_size=filter_size_3,stride=1)\n",
    "        H_out_4, W_out_4 = conv_output_shape_dynamic((H_out_3, W_out_3), kernel_size=filter_size_2,stride=stride)\n",
    "        \n",
    "        print(\"imgres\",image_resolution_x,image_resolution_y)\n",
    "        print(\"H_out_1, W_out_1\",H_out_1, W_out_1)\n",
    "        print(\"H_out_2, W_out_2\",H_out_2, W_out_2)\n",
    "        print(\"H_out_3, W_out_4\",H_out_3, W_out_3)\n",
    "        print(\"H_out_4, W_out_4\",H_out_4, W_out_4)\n",
    "        print(\"outputconv2\",output_conv_2)\n",
    "        self.conv_output_size = H_out_4 * W_out_4\n",
    "\n",
    "        #Fully connected layers and apply dropout\n",
    "        self.fc1 = nn.Linear(output_conv_2*self.conv_output_size, output_FC_1)\n",
    "        if(dropout_probab>0): self.dropout1 = nn.Dropout(dropout_probab)\n",
    "        self.fc2 = nn.Linear(output_FC_1, output_FC_2)\n",
    "        if(dropout_probab>0): self.dropout1 = nn.Dropout(dropout_probab)\n",
    "        self.fc3 = nn.Linear(output_FC_2, final_FCLayer_outputs)\n",
    "        \n",
    "        # compute the total number of parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(self.name + ': total params:', total_params)\n",
    "        self.totalparams=total_params\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"Input shape:\", x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #print(\"After conv1 and pooling shape:\", x.shape)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #print(\"After conv2 and pooling shape:\", x.shape)\n",
    "        #-1 takes the batch size\n",
    "        x = x.view(-1,self.output_conv_2*self.conv_output_size)\n",
    "        #print(\"After flattening shape:\", x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(\"After fc1 shape:\", x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(\"After fc2 shape:\", x.shape)\n",
    "        x = self.fc3(x)\n",
    "        #print(\"Output shape:\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default hyperparameters\n",
    "\n",
    "filter_size_1=(2, 2)\n",
    "filter_size_2=(2, 3)\n",
    "filter_size_3=(2, 3)\n",
    "\n",
    "stride=2\n",
    "\n",
    "output_conv_1=40\n",
    "output_conv_2=12\n",
    "output_FC_1=100\n",
    "output_FC_2=70\n",
    "final_FCLayer_output=1\n",
    "\n",
    "learning_rate=0.001\n",
    "momentum = 0.9\n",
    "\n",
    "dropout_probab=0\n",
    "\n",
    "num_epochs_input = 20\n",
    "\n",
    "transform = SetTransform(normalize_ftor=0.5, resolution_x=32, resolution_y=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Convos & dropoutP: 40 12 0\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 12\n",
      "Classification Net: total params: 60733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x00000237CAC6C580>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(name='Classification Net', filter_size_1=filter_size_1, filter_size_2=filter_size_2,\n",
    "            filter_size_3=filter_size_3, stride=stride,\n",
    "            image_resolution_x=32,image_resolution_y=32,\n",
    "            output_conv_1=output_conv_1, output_conv_2=output_conv_2,\n",
    "            output_FC_1=output_FC_1, output_FC_2=output_FC_2,\n",
    "            final_FCLayer_outputs=final_FCLayer_output,\n",
    "            dropout_probab=dropout_probab)\n",
    "\n",
    "net.to(device)\n",
    "net.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train params: 0.001 0.9\n",
      "[1,    50] Cum loss: 0.422532\n",
      "[1,   100] Cum loss: 0.020577\n",
      "[1,   150] Cum loss: 0.012949\n",
      "[2,    50] Cum loss: 0.005529\n",
      "[2,   100] Cum loss: 0.003281\n",
      "[2,   150] Cum loss: 0.002941\n",
      "[3,    50] Cum loss: 0.002459\n",
      "[3,   100] Cum loss: 0.001525\n",
      "[3,   150] Cum loss: 0.001348\n",
      "[4,    50] Cum loss: 0.002006\n",
      "[4,   100] Cum loss: 0.001162\n",
      "[4,   150] Cum loss: 0.001214\n",
      "[5,    50] Cum loss: 0.001000\n",
      "[5,   100] Cum loss: 0.000983\n",
      "[5,   150] Cum loss: 0.001023\n",
      "[6,    50] Cum loss: 0.000840\n",
      "[6,   100] Cum loss: 0.001063\n",
      "[6,   150] Cum loss: 0.000783\n",
      "[7,    50] Cum loss: 0.000632\n",
      "[7,   100] Cum loss: 0.000698\n",
      "[7,   150] Cum loss: 0.000932\n",
      "[8,    50] Cum loss: 0.000795\n",
      "[8,   100] Cum loss: 0.001160\n",
      "[8,   150] Cum loss: 0.000605\n",
      "[9,    50] Cum loss: 0.000486\n",
      "[9,   100] Cum loss: 0.000661\n",
      "[9,   150] Cum loss: 0.000858\n",
      "[10,    50] Cum loss: 0.000975\n",
      "[10,   100] Cum loss: 0.000842\n",
      "[10,   150] Cum loss: 0.000799\n",
      "[11,    50] Cum loss: 0.000773\n",
      "[11,   100] Cum loss: 0.000654\n",
      "[11,   150] Cum loss: 0.000381\n",
      "[12,    50] Cum loss: 0.000528\n",
      "[12,   100] Cum loss: 0.000774\n",
      "[12,   150] Cum loss: 0.000479\n",
      "[13,    50] Cum loss: 0.000771\n",
      "[13,   100] Cum loss: 0.000617\n",
      "[13,   150] Cum loss: 0.000271\n",
      "[14,    50] Cum loss: 0.000774\n",
      "[14,   100] Cum loss: 0.001159\n",
      "[14,   150] Cum loss: 0.000809\n",
      "[15,    50] Cum loss: 0.000341\n",
      "[15,   100] Cum loss: 0.000367\n",
      "[15,   150] Cum loss: 0.000469\n",
      "[16,    50] Cum loss: 0.000359\n",
      "[16,   100] Cum loss: 0.000595\n",
      "[16,   150] Cum loss: 0.000492\n",
      "[17,    50] Cum loss: 0.000694\n",
      "[17,   100] Cum loss: 0.000524\n",
      "[17,   150] Cum loss: 0.000434\n",
      "[18,    50] Cum loss: 0.000443\n",
      "[18,   100] Cum loss: 0.000395\n",
      "[18,   150] Cum loss: 0.000659\n",
      "[19,    50] Cum loss: 0.000867\n",
      "[19,   100] Cum loss: 0.000349\n",
      "[19,   150] Cum loss: 0.000415\n",
      "[20,    50] Cum loss: 0.000850\n",
      "[20,   100] Cum loss: 0.000249\n",
      "[20,   150] Cum loss: 0.000195\n",
      "tensor(4.2152e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#print running_loss every [x] mini-batches\n",
    "mini_batch_running_loss_check = 50\n",
    "\n",
    "def Train(learning_rate=learning_rate,momentum=momentum,\n",
    "          train_loader=train_loader, net=net):\n",
    "    \n",
    "    print(\"Train params:\",learning_rate,momentum)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    for epoch in range(num_epochs_input):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            #get the inputs; data is a list of [inputs, labels]\n",
    "            # print(\"image type\",type(data[0]), \"shape\", data[0].shape)\n",
    "            # print(\"image size\",data[0].numel())\n",
    "            # print(\"label type\",type(data[1]), \"shape\", data[1].shape)\n",
    "            # print(\"label size\",data[1].numel())\n",
    "            #print(\"**zero:\",data[0])\n",
    "            #print(\"type\",type(data[1]),\"**one:\", data[1])\n",
    "            #print(\"label pre\",data[1])\n",
    "            #data[1]=data[1].type(torch.LongTensor)\n",
    "            #print(\"label post\",data[1])\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            #print(\"at epoch\",epoch,\"at i\",i,\"emotion\",labels_emotion,\"person\",labels_person)\n",
    "            \n",
    "            # for e in data[1]:\n",
    "            #     print(\"label\",e.item())\n",
    "            #print(\"label\",data[1].item())\n",
    "            #labels = labels.long()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            #print(\"outputs\",outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print optimizer's state_dict\n",
    "                # print(\"Optimizer's state_dict:\")\n",
    "                # for var_name in optimizer.state_dict():\n",
    "                #     print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % mini_batch_running_loss_check == (mini_batch_running_loss_check-1):    # print every 20 mini-batches\n",
    "                    print('[%d, %5d] Cum loss: %.6f' %\n",
    "                        (epoch + 1, i + 1, running_loss / mini_batch_running_loss_check)) \n",
    "                    running_loss = 0.0\n",
    "\n",
    "                if loss.item() < 0.000001:\n",
    "                    print(\"Loss is less than 0.0001. Stopping training.\")\n",
    "                    return loss\n",
    "                \n",
    "    return loss\n",
    "\n",
    "loss = Train()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CNN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(test_loader=test_loader, net=net):\n",
    "    predicted = []\n",
    "    actual = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        predicted.append(outputs)\n",
    "        actual.append(labels)\n",
    "\n",
    "        predicted_rounded = torch.round(outputs.data * 100) / 100\n",
    "        actual_rounded = torch.round(labels.data * 100) / 100\n",
    "        correct += torch.sum(torch.abs(predicted_rounded - actual_rounded) <= 0.01).item()\n",
    "        total += len(predicted_rounded)\n",
    "    #Pct correct pred\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"Percentage of predictions within 2 decimal places: {accuracy:.2f}%\")\n",
    "\n",
    "    #print(predicted,actual)\n",
    "    return predicted,actual,accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of predictions within 2 decimal places: 2.85%\n",
      "     Predicted    Actual Percentage Difference\n",
      "0    -1.704265 -1.728527                1.404%\n",
      "1    -1.703422 -1.728527                1.452%\n",
      "2    -1.711620 -1.728527                0.978%\n",
      "3    -1.710918 -1.728527                1.019%\n",
      "4    -1.698964 -1.728527                1.710%\n",
      "..         ...       ...                   ...\n",
      "486  -1.698739 -1.728527                1.723%\n",
      "487  -1.685047 -1.728527                2.515%\n",
      "488  -1.724468 -1.728527                0.235%\n",
      "489  -1.714897 -1.728527                0.789%\n",
      "490  -1.700616 -1.728527                1.615%\n",
      "\n",
      "[491 rows x 3 columns]\n",
      "Accuracy: 2.851 Average Percentage Difference: 1.377% for timeseries of 491 datapoints across 5 features: ['Open', 'High', 'Low', 'Close', 'Adj Close'].\n"
     ]
    }
   ],
   "source": [
    "predicted, actual, accuracy = Test()\n",
    "\n",
    "predicted = torch.cat(predicted)\n",
    "actual = torch.cat(actual)\n",
    "absolute_diff = torch.abs(predicted - actual)\n",
    "percentage_diff = (absolute_diff / torch.abs(actual)) * 100\n",
    "\n",
    "predicted_np = predicted.cpu().detach().numpy()\n",
    "actual_np = actual.cpu().detach().numpy()\n",
    "percentage_diff_np = percentage_diff.cpu().detach().numpy()\n",
    "\n",
    "# Create a dictionary with the data\n",
    "data = {\n",
    "    'Predicted': predicted_np.flatten(),\n",
    "    'Actual': actual_np.flatten(),\n",
    "    'Percentage Difference': percentage_diff_np.flatten()\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df['Percentage Difference'] = df['Percentage Difference'].map(lambda x: f\"{x:.3f}%\")\n",
    "\n",
    "print(df)\n",
    "\n",
    "df['Percentage Difference'] = df['Percentage Difference'].str.rstrip('%').astype(float)\n",
    "\n",
    "average_percentage_diff = df['Percentage Difference'].mean()\n",
    "print(f\"Accuracy: {accuracy:.3f} Average Percentage Difference: {average_percentage_diff:.3f}% for timeseries of {len(df)} datapoints across {len(cols_used)} features: {cols_used}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization for CNN Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Optimization(output_conv_1, \n",
    "                     output_conv_2,\n",
    "                     dropout_probab,\n",
    "                     learning_rate,\n",
    "                     train_loader,test_loader):\n",
    "    \n",
    "    output_conv_1 = int(output_conv_1)\n",
    "    output_conv_2 = int(output_conv_2)\n",
    "    \n",
    "    net = Net(output_conv_1=output_conv_1, output_conv_2=output_conv_2,dropout_probab=dropout_probab)\n",
    "    net.to(device)\n",
    "\n",
    "    Train(learning_rate=learning_rate, train_loader=train_loader, net=net)\n",
    "\n",
    "    predicted,actual,accuracy = Test(test_loader=test_loader,net=net)\n",
    "    print(\"accuracy received\",accuracy)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from functools import partial\n",
    "\n",
    "def Optimize():\n",
    "    \n",
    "    Generate_Train_And_Test_Loaders(feature_image_dataset_list_f32,labels_scaled_list_f32, train_shuffle=False, batch_size=10)\n",
    "    \n",
    "    cnn_correct_pct = partial(CNN_Optimization, train_loader = train_loader, test_loader = test_loader)\n",
    "\n",
    "    # Bounded region of parameter space\n",
    "    pbounds = {'output_conv_1': (40, 80), 'output_conv_2': (8, 16), 'learning_rate': (0.001, 0.01), 'dropout_probab': (0.0, 0.5)}\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=cnn_correct_pct,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1,\n",
    "        )\n",
    "\n",
    "    #n_iter:steps of bayesian optimization you want to perform\n",
    "    #init_points:steps of random exploration\n",
    "    optimizer.maximize(init_points=10, n_iter=10,)\n",
    "\n",
    "    for i, res in enumerate(optimizer.res):\n",
    "        print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "    print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 491, 32, 32) 5\n",
      "input_shape torch.Size([10, 1, 32, 32]) label_shape torch.Size([10, 1])\n",
      "|   iter    |  target   | dropou... | learni... | output... | output... |\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "Convos & dropoutP: 40 10 0.208511002351287\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 10\n",
      "LeNet5Blend: total params: 63379\n",
      "Train params: 0.007482920440979424 0.9\n",
      "[1,    50] Cum loss: 2.626901\n",
      "[1,   100] Cum loss: 0.010061\n",
      "Loss is less than 0.0001. Stopping training.\n",
      "Percentage of predictions within 2 decimal places: 100.00%\n",
      "accuracy received 100.0\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m100.0    \u001b[0m | \u001b[0m0.2085   \u001b[0m | \u001b[0m0.007483 \u001b[0m | \u001b[0m40.0     \u001b[0m | \u001b[0m10.42    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 47 10 0.07337794540855652\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 10\n",
      "LeNet5Blend: total params: 63834\n",
      "Train params: 0.0018310473529191804 0.9\n",
      "[1,    50] Cum loss: 0.245131\n",
      "[1,   100] Cum loss: 0.017393\n",
      "[1,   150] Cum loss: 0.012978\n",
      "[2,    50] Cum loss: 0.006604\n",
      "[2,   100] Cum loss: 0.005344\n",
      "[2,   150] Cum loss: 0.003504\n",
      "[3,    50] Cum loss: 0.002392\n",
      "[3,   100] Cum loss: 0.002180\n",
      "[3,   150] Cum loss: 0.002919\n",
      "[4,    50] Cum loss: 0.001256\n",
      "[4,   100] Cum loss: 0.001165\n",
      "[4,   150] Cum loss: 0.001879\n",
      "[5,    50] Cum loss: 0.001622\n",
      "[5,   100] Cum loss: 0.001167\n",
      "[5,   150] Cum loss: 0.001724\n",
      "[6,    50] Cum loss: 0.001813\n",
      "[6,   100] Cum loss: 0.001432\n",
      "[6,   150] Cum loss: 0.001455\n",
      "[7,    50] Cum loss: 0.001236\n",
      "[7,   100] Cum loss: 0.000791\n",
      "[7,   150] Cum loss: 0.000531\n",
      "[8,    50] Cum loss: 0.001505\n",
      "[8,   100] Cum loss: 0.000492\n",
      "[8,   150] Cum loss: 0.001192\n",
      "[9,    50] Cum loss: 0.001370\n",
      "[9,   100] Cum loss: 0.001439\n",
      "[9,   150] Cum loss: 0.001986\n",
      "[10,    50] Cum loss: 0.000913\n",
      "[10,   100] Cum loss: 0.001034\n",
      "[10,   150] Cum loss: 0.001407\n",
      "[11,    50] Cum loss: 0.000994\n",
      "[11,   100] Cum loss: 0.000434\n",
      "[11,   150] Cum loss: 0.001984\n",
      "[12,    50] Cum loss: 0.002021\n",
      "[12,   100] Cum loss: 0.000951\n",
      "[12,   150] Cum loss: 0.001073\n",
      "[13,    50] Cum loss: 0.000425\n",
      "[13,   100] Cum loss: 0.000693\n",
      "[13,   150] Cum loss: 0.001283\n",
      "[14,    50] Cum loss: 0.000377\n",
      "[14,   100] Cum loss: 0.000828\n",
      "[14,   150] Cum loss: 0.001512\n",
      "[15,    50] Cum loss: 0.001524\n",
      "[15,   100] Cum loss: 0.000724\n",
      "[15,   150] Cum loss: 0.001188\n",
      "[16,    50] Cum loss: 0.000732\n",
      "[16,   100] Cum loss: 0.001721\n",
      "[16,   150] Cum loss: 0.000285\n",
      "[17,    50] Cum loss: 0.000692\n",
      "[17,   100] Cum loss: 0.000782\n",
      "[17,   150] Cum loss: 0.000575\n",
      "[18,    50] Cum loss: 0.000442\n",
      "[18,   100] Cum loss: 0.001724\n",
      "[18,   150] Cum loss: 0.000461\n",
      "[19,    50] Cum loss: 0.000528\n",
      "[19,   100] Cum loss: 0.000842\n",
      "[19,   150] Cum loss: 0.000695\n",
      "[20,    50] Cum loss: 0.000511\n",
      "[20,   100] Cum loss: 0.001364\n",
      "[20,   150] Cum loss: 0.001019\n",
      "Percentage of predictions within 2 decimal places: 7.33%\n",
      "accuracy received 7.3319755600814664\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m7.332    \u001b[0m | \u001b[0m0.07338  \u001b[0m | \u001b[0m0.001831 \u001b[0m | \u001b[0m47.45    \u001b[0m | \u001b[0m10.76    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 56 13 0.19838373711533497\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 13\n",
      "LeNet5Blend: total params: 80550\n",
      "Train params: 0.005849350606030213 0.9\n",
      "[1,    50] Cum loss: 0.232164\n",
      "[1,   100] Cum loss: 0.021619\n",
      "[1,   150] Cum loss: 0.011401\n",
      "[2,    50] Cum loss: 0.004466\n",
      "[2,   100] Cum loss: 0.003350\n",
      "[2,   150] Cum loss: 0.001320\n",
      "[3,    50] Cum loss: 0.000928\n",
      "[3,   100] Cum loss: 0.001873\n",
      "[3,   150] Cum loss: 0.002506\n",
      "[4,    50] Cum loss: 0.001729\n",
      "[4,   100] Cum loss: 0.002721\n",
      "[4,   150] Cum loss: 0.004886\n",
      "[5,    50] Cum loss: 0.001152\n",
      "[5,   100] Cum loss: 0.001677\n",
      "[5,   150] Cum loss: 0.001077\n",
      "[6,    50] Cum loss: 0.001207\n",
      "[6,   100] Cum loss: 0.002286\n",
      "[6,   150] Cum loss: 0.001654\n",
      "[7,    50] Cum loss: 0.002707\n",
      "[7,   100] Cum loss: 0.001126\n",
      "[7,   150] Cum loss: 0.000787\n",
      "[8,    50] Cum loss: 0.000675\n",
      "[8,   100] Cum loss: 0.000577\n",
      "[8,   150] Cum loss: 0.000972\n",
      "[9,    50] Cum loss: 0.001113\n",
      "[9,   100] Cum loss: 0.001532\n",
      "[9,   150] Cum loss: 0.002418\n",
      "[10,    50] Cum loss: 0.001787\n",
      "[10,   100] Cum loss: 0.000780\n",
      "[10,   150] Cum loss: 0.000785\n",
      "[11,    50] Cum loss: 0.000356\n",
      "[11,   100] Cum loss: 0.000369\n",
      "[11,   150] Cum loss: 0.000420\n",
      "[12,    50] Cum loss: 0.001004\n",
      "[12,   100] Cum loss: 0.000382\n",
      "[12,   150] Cum loss: 0.000275\n",
      "[13,    50] Cum loss: 0.000372\n",
      "[13,   100] Cum loss: 0.000623\n",
      "[13,   150] Cum loss: 0.000611\n",
      "[14,    50] Cum loss: 0.001340\n",
      "[14,   100] Cum loss: 0.000637\n",
      "[14,   150] Cum loss: 0.000950\n",
      "[15,    50] Cum loss: 0.000896\n",
      "[15,   100] Cum loss: 0.000397\n",
      "[15,   150] Cum loss: 0.000428\n",
      "[16,    50] Cum loss: 0.000418\n",
      "[16,   100] Cum loss: 0.000659\n",
      "[16,   150] Cum loss: 0.000537\n",
      "[17,    50] Cum loss: 0.000294\n",
      "[17,   100] Cum loss: 0.000203\n",
      "[17,   150] Cum loss: 0.000421\n",
      "[18,    50] Cum loss: 0.000256\n",
      "[18,   100] Cum loss: 0.000453\n",
      "[18,   150] Cum loss: 0.000963\n",
      "[19,    50] Cum loss: 0.000464\n",
      "[19,   100] Cum loss: 0.000461\n",
      "[19,   150] Cum loss: 0.000328\n",
      "[20,    50] Cum loss: 0.000408\n",
      "[20,   100] Cum loss: 0.000452\n",
      "[20,   150] Cum loss: 0.000242\n",
      "Percentage of predictions within 2 decimal places: 0.00%\n",
      "accuracy received 0.0\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.1984   \u001b[0m | \u001b[0m0.005849 \u001b[0m | \u001b[0m56.77    \u001b[0m | \u001b[0m13.48    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 41 13 0.10222612486575872\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 13\n",
      "LeNet5Blend: total params: 79305\n",
      "Train params: 0.00890305692751851 0.9\n",
      "[1,    50] Cum loss: 4.412610\n",
      "[1,   100] Cum loss: 0.013511\n",
      "[1,   150] Cum loss: 0.005648\n",
      "[2,    50] Cum loss: 0.003556\n",
      "[2,   100] Cum loss: 0.003256\n",
      "[2,   150] Cum loss: 0.001733\n",
      "[3,    50] Cum loss: 0.000933\n",
      "[3,   100] Cum loss: 0.000626\n",
      "[3,   150] Cum loss: 0.001181\n",
      "[4,    50] Cum loss: 0.000669\n",
      "[4,   100] Cum loss: 0.000533\n",
      "[4,   150] Cum loss: 0.000237\n",
      "[5,    50] Cum loss: 0.000130\n",
      "[5,   100] Cum loss: 0.000157\n",
      "[5,   150] Cum loss: 0.000126\n",
      "[6,    50] Cum loss: 0.000042\n",
      "[6,   100] Cum loss: 0.000041\n",
      "[6,   150] Cum loss: 0.000028\n",
      "Loss is less than 0.0001. Stopping training.\n",
      "Percentage of predictions within 2 decimal places: 94.50%\n",
      "accuracy received 94.5010183299389\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m94.5     \u001b[0m | \u001b[0m0.1022   \u001b[0m | \u001b[0m0.008903 \u001b[0m | \u001b[0m41.1     \u001b[0m | \u001b[0m13.36    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 45 9 0.20865240118356349\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 9\n",
      "LeNet5Blend: total params: 58393\n",
      "Train params: 0.006028208456011766 0.9\n",
      "[1,    50] Cum loss: 0.610468\n",
      "[1,   100] Cum loss: 0.024796\n",
      "[1,   150] Cum loss: 0.018801\n",
      "[2,    50] Cum loss: 0.009746\n",
      "[2,   100] Cum loss: 0.009055\n",
      "[2,   150] Cum loss: 0.005324\n",
      "[3,    50] Cum loss: 0.006833\n",
      "[3,   100] Cum loss: 0.004577\n",
      "[3,   150] Cum loss: 0.003442\n",
      "[4,    50] Cum loss: 0.001500\n",
      "[4,   100] Cum loss: 0.001834\n",
      "[4,   150] Cum loss: 0.001329\n",
      "[5,    50] Cum loss: 0.001183\n",
      "[5,   100] Cum loss: 0.001389\n",
      "[5,   150] Cum loss: 0.001034\n",
      "[6,    50] Cum loss: 0.001696\n",
      "[6,   100] Cum loss: 0.000748\n",
      "[6,   150] Cum loss: 0.001730\n",
      "[7,    50] Cum loss: 0.001007\n",
      "[7,   100] Cum loss: 0.000614\n",
      "[7,   150] Cum loss: 0.000727\n",
      "[8,    50] Cum loss: 0.001255\n",
      "[8,   100] Cum loss: 0.001066\n",
      "[8,   150] Cum loss: 0.000619\n",
      "[9,    50] Cum loss: 0.000542\n",
      "[9,   100] Cum loss: 0.001028\n",
      "[9,   150] Cum loss: 0.000980\n",
      "[10,    50] Cum loss: 0.000416\n",
      "[10,   100] Cum loss: 0.000947\n",
      "[10,   150] Cum loss: 0.002265\n",
      "[11,    50] Cum loss: 0.000914\n",
      "[11,   100] Cum loss: 0.000344\n",
      "[11,   150] Cum loss: 0.000707\n",
      "[12,    50] Cum loss: 0.001534\n",
      "[12,   100] Cum loss: 0.000589\n",
      "[12,   150] Cum loss: 0.000380\n",
      "[13,    50] Cum loss: 0.000342\n",
      "[13,   100] Cum loss: 0.000400\n",
      "[13,   150] Cum loss: 0.000660\n",
      "[14,    50] Cum loss: 0.000477\n",
      "[14,   100] Cum loss: 0.000395\n",
      "[14,   150] Cum loss: 0.000525\n",
      "[15,    50] Cum loss: 0.000280\n",
      "[15,   100] Cum loss: 0.000295\n",
      "[15,   150] Cum loss: 0.000378\n",
      "[16,    50] Cum loss: 0.000526\n",
      "[16,   100] Cum loss: 0.000216\n",
      "[16,   150] Cum loss: 0.000228\n",
      "[17,    50] Cum loss: 0.001346\n",
      "[17,   100] Cum loss: 0.000417\n",
      "[17,   150] Cum loss: 0.000476\n",
      "[18,    50] Cum loss: 0.000315\n",
      "[18,   100] Cum loss: 0.000157\n",
      "[18,   150] Cum loss: 0.000514\n",
      "[19,    50] Cum loss: 0.000275\n",
      "[19,   100] Cum loss: 0.000487\n",
      "[19,   150] Cum loss: 0.001923\n",
      "[20,    50] Cum loss: 0.000240\n",
      "[20,   100] Cum loss: 0.000233\n",
      "[20,   150] Cum loss: 0.000289\n",
      "Percentage of predictions within 2 decimal places: 89.00%\n",
      "accuracy received 89.0020366598778\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m89.0     \u001b[0m | \u001b[0m0.2087   \u001b[0m | \u001b[0m0.006028 \u001b[0m | \u001b[0m45.62    \u001b[0m | \u001b[0m9.585    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 52 13 0.40037228433776834\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 13\n",
      "LeNet5Blend: total params: 80218\n",
      "Train params: 0.00971435418147458 0.9\n",
      "[1,    50] Cum loss: 3.223274\n",
      "[1,   100] Cum loss: 0.025359\n",
      "[1,   150] Cum loss: 0.012518\n",
      "[2,    50] Cum loss: 0.007474\n",
      "[2,   100] Cum loss: 0.005604\n",
      "[2,   150] Cum loss: 0.008436\n",
      "[3,    50] Cum loss: 0.002992\n",
      "[3,   100] Cum loss: 0.002805\n",
      "[3,   150] Cum loss: 0.001834\n",
      "[4,    50] Cum loss: 0.003633\n",
      "[4,   100] Cum loss: 0.002919\n",
      "[4,   150] Cum loss: 0.002238\n",
      "[5,    50] Cum loss: 0.001170\n",
      "[5,   100] Cum loss: 0.003608\n",
      "[5,   150] Cum loss: 0.002382\n",
      "[6,    50] Cum loss: 0.000730\n",
      "[6,   100] Cum loss: 0.001214\n",
      "[6,   150] Cum loss: 0.001638\n",
      "[7,    50] Cum loss: 0.004308\n",
      "[7,   100] Cum loss: 0.001485\n",
      "[7,   150] Cum loss: 0.001209\n",
      "[8,    50] Cum loss: 0.002480\n",
      "[8,   100] Cum loss: 0.000814\n",
      "[8,   150] Cum loss: 0.000714\n",
      "[9,    50] Cum loss: 0.001795\n",
      "[9,   100] Cum loss: 0.001809\n",
      "[9,   150] Cum loss: 0.001296\n",
      "[10,    50] Cum loss: 0.002782\n",
      "[10,   100] Cum loss: 0.001459\n",
      "[10,   150] Cum loss: 0.000740\n",
      "[11,    50] Cum loss: 0.001176\n",
      "[11,   100] Cum loss: 0.004392\n",
      "[11,   150] Cum loss: 0.001230\n",
      "[12,    50] Cum loss: 0.000678\n",
      "[12,   100] Cum loss: 0.003684\n",
      "[12,   150] Cum loss: 0.003970\n",
      "[13,    50] Cum loss: 0.001371\n",
      "[13,   100] Cum loss: 0.002142\n",
      "[13,   150] Cum loss: 0.000677\n",
      "[14,    50] Cum loss: 0.003818\n",
      "[14,   100] Cum loss: 0.004826\n",
      "[14,   150] Cum loss: 0.000836\n",
      "[15,    50] Cum loss: 0.042187\n",
      "[15,   100] Cum loss: 0.001573\n",
      "[15,   150] Cum loss: 0.000523\n",
      "[16,    50] Cum loss: 0.000332\n",
      "[16,   100] Cum loss: 0.000554\n",
      "[16,   150] Cum loss: 0.000428\n",
      "[17,    50] Cum loss: 0.000291\n",
      "[17,   100] Cum loss: 0.000350\n",
      "[17,   150] Cum loss: 0.000456\n",
      "[18,    50] Cum loss: 0.000394\n",
      "[18,   100] Cum loss: 0.000538\n",
      "[18,   150] Cum loss: 0.000415\n",
      "[19,    50] Cum loss: 0.000236\n",
      "[19,   100] Cum loss: 0.000512\n",
      "[19,   150] Cum loss: 0.001270\n",
      "[20,    50] Cum loss: 0.000982\n",
      "[20,   100] Cum loss: 0.000646\n",
      "[20,   150] Cum loss: 0.000359\n",
      "Percentage of predictions within 2 decimal places: 0.00%\n",
      "accuracy received 0.0\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.4004   \u001b[0m | \u001b[0m0.009714 \u001b[0m | \u001b[0m52.54    \u001b[0m | \u001b[0m13.54    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 43 8 0.43819457614801915\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 8\n",
      "LeNet5Blend: total params: 52976\n",
      "Train params: 0.009051459971534628 0.9\n",
      "[1,    50] Cum loss: 2.325716\n",
      "[1,   100] Cum loss: 0.004017\n",
      "Loss is less than 0.0001. Stopping training.\n",
      "Percentage of predictions within 2 decimal places: 100.00%\n",
      "accuracy received 100.0\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m100.0    \u001b[0m | \u001b[0m0.4382   \u001b[0m | \u001b[0m0.009051 \u001b[0m | \u001b[0m43.4     \u001b[0m | \u001b[0m8.312    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 43 11 0.08491520978228445\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 11\n",
      "LeNet5Blend: total params: 68873\n",
      "Train params: 0.008903282530864719 0.9\n",
      "[1,    50] Cum loss: 0.831791\n",
      "[1,   100] Cum loss: 0.002501\n",
      "Loss is less than 0.0001. Stopping training.\n",
      "Percentage of predictions within 2 decimal places: 100.00%\n",
      "accuracy received 100.0\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m100.0    \u001b[0m | \u001b[0m0.08492  \u001b[0m | \u001b[0m0.008903 \u001b[0m | \u001b[0m43.93    \u001b[0m | \u001b[0m11.37    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 67 10 0.47894476507525097\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 10\n",
      "LeNet5Blend: total params: 65134\n",
      "Train params: 0.005798487564757154 0.9\n",
      "[1,    50] Cum loss: 0.538176\n",
      "[1,   100] Cum loss: 0.026294\n",
      "[1,   150] Cum loss: 0.017831\n",
      "[2,    50] Cum loss: 0.003586\n",
      "[2,   100] Cum loss: 0.002483\n",
      "[2,   150] Cum loss: 0.001627\n",
      "[3,    50] Cum loss: 0.001806\n",
      "[3,   100] Cum loss: 0.001357\n",
      "[3,   150] Cum loss: 0.000981\n",
      "[4,    50] Cum loss: 0.001090\n",
      "[4,   100] Cum loss: 0.002277\n",
      "[4,   150] Cum loss: 0.001005\n",
      "[5,    50] Cum loss: 0.001492\n",
      "[5,   100] Cum loss: 0.001876\n",
      "[5,   150] Cum loss: 0.000890\n",
      "[6,    50] Cum loss: 0.000845\n",
      "[6,   100] Cum loss: 0.000865\n",
      "[6,   150] Cum loss: 0.001817\n",
      "[7,    50] Cum loss: 0.001082\n",
      "[7,   100] Cum loss: 0.000291\n",
      "[7,   150] Cum loss: 0.000473\n",
      "[8,    50] Cum loss: 0.000437\n",
      "[8,   100] Cum loss: 0.000258\n",
      "[8,   150] Cum loss: 0.001928\n",
      "[9,    50] Cum loss: 0.001318\n",
      "[9,   100] Cum loss: 0.000469\n",
      "[9,   150] Cum loss: 0.000504\n",
      "[10,    50] Cum loss: 0.000616\n",
      "[10,   100] Cum loss: 0.002916\n",
      "[10,   150] Cum loss: 0.001610\n",
      "[11,    50] Cum loss: 0.001129\n",
      "[11,   100] Cum loss: 0.000525\n",
      "[11,   150] Cum loss: 0.001580\n",
      "[12,    50] Cum loss: 0.000384\n",
      "[12,   100] Cum loss: 0.000861\n",
      "[12,   150] Cum loss: 0.001408\n",
      "[13,    50] Cum loss: 0.000695\n",
      "[13,   100] Cum loss: 0.000393\n",
      "[13,   150] Cum loss: 0.000429\n",
      "[14,    50] Cum loss: 0.000896\n",
      "[14,   100] Cum loss: 0.000196\n",
      "[14,   150] Cum loss: 0.000512\n",
      "[15,    50] Cum loss: 0.000297\n",
      "[15,   100] Cum loss: 0.000471\n",
      "[15,   150] Cum loss: 0.000243\n",
      "[16,    50] Cum loss: 0.000320\n",
      "[16,   100] Cum loss: 0.000447\n",
      "[16,   150] Cum loss: 0.000253\n",
      "[17,    50] Cum loss: 0.006724\n",
      "[17,   100] Cum loss: 0.000324\n",
      "[17,   150] Cum loss: 0.000734\n",
      "[18,    50] Cum loss: 0.000193\n",
      "[18,   100] Cum loss: 0.000158\n",
      "[18,   150] Cum loss: 0.000136\n",
      "[19,    50] Cum loss: 0.000385\n",
      "[19,   100] Cum loss: 0.000096\n",
      "[19,   150] Cum loss: 0.000318\n",
      "[20,    50] Cum loss: 0.000171\n",
      "[20,   100] Cum loss: 0.000488\n",
      "[20,   150] Cum loss: 0.000713\n",
      "Percentage of predictions within 2 decimal places: 72.30%\n",
      "accuracy received 72.30142566191446\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m72.3     \u001b[0m | \u001b[0m0.4789   \u001b[0m | \u001b[0m0.005798 \u001b[0m | \u001b[0m67.68    \u001b[0m | \u001b[0m10.52    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 40 14 0.34325046384079183\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 14\n",
      "LeNet5Blend: total params: 84503\n",
      "Train params: 0.008511631047076357 0.9\n",
      "[1,    50] Cum loss: 0.797492\n",
      "[1,   100] Cum loss: 0.002391\n",
      "Loss is less than 0.0001. Stopping training.\n",
      "Percentage of predictions within 2 decimal places: 100.00%\n",
      "accuracy received 100.0\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m100.0    \u001b[0m | \u001b[0m0.3433   \u001b[0m | \u001b[0m0.008512 \u001b[0m | \u001b[0m40.73    \u001b[0m | \u001b[0m14.0     \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 75 8 0.5\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 8\n",
      "LeNet5Blend: total params: 54672\n",
      "Train params: 0.0067450735094932565 0.9\n",
      "[1,    50] Cum loss: 1.321032\n",
      "Loss is less than 0.0001. Stopping training.\n",
      "Percentage of predictions within 2 decimal places: 0.00%\n",
      "accuracy received 0.0\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.006745 \u001b[0m | \u001b[0m75.85    \u001b[0m | \u001b[0m8.362    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 68 16 0.0\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 16\n",
      "LeNet5Blend: total params: 97893\n",
      "Train params: 0.001 0.9\n",
      "[1,    50] Cum loss: 0.233950\n",
      "[1,   100] Cum loss: 0.022067\n",
      "[1,   150] Cum loss: 0.015721\n",
      "[2,    50] Cum loss: 0.008655\n",
      "[2,   100] Cum loss: 0.006924\n",
      "[2,   150] Cum loss: 0.004351\n",
      "[3,    50] Cum loss: 0.004724\n",
      "[3,   100] Cum loss: 0.003485\n",
      "[3,   150] Cum loss: 0.002693\n",
      "[4,    50] Cum loss: 0.001690\n",
      "[4,   100] Cum loss: 0.002029\n",
      "[4,   150] Cum loss: 0.001466\n",
      "[5,    50] Cum loss: 0.002851\n",
      "[5,   100] Cum loss: 0.001397\n",
      "[5,   150] Cum loss: 0.001368\n",
      "[6,    50] Cum loss: 0.002805\n",
      "[6,   100] Cum loss: 0.000870\n",
      "[6,   150] Cum loss: 0.001020\n",
      "[7,    50] Cum loss: 0.000801\n",
      "[7,   100] Cum loss: 0.001168\n",
      "[7,   150] Cum loss: 0.001885\n",
      "[8,    50] Cum loss: 0.001762\n",
      "[8,   100] Cum loss: 0.001016\n",
      "[8,   150] Cum loss: 0.000702\n",
      "[9,    50] Cum loss: 0.001510\n",
      "[9,   100] Cum loss: 0.001209\n",
      "[9,   150] Cum loss: 0.001804\n",
      "[10,    50] Cum loss: 0.000500\n",
      "[10,   100] Cum loss: 0.001412\n",
      "[10,   150] Cum loss: 0.000801\n",
      "[11,    50] Cum loss: 0.001390\n",
      "[11,   100] Cum loss: 0.002361\n",
      "[11,   150] Cum loss: 0.001457\n",
      "[12,    50] Cum loss: 0.000647\n",
      "[12,   100] Cum loss: 0.000975\n",
      "[12,   150] Cum loss: 0.000962\n",
      "[13,    50] Cum loss: 0.000663\n",
      "[13,   100] Cum loss: 0.001057\n",
      "[13,   150] Cum loss: 0.000385\n",
      "[14,    50] Cum loss: 0.002586\n",
      "[14,   100] Cum loss: 0.001934\n",
      "[14,   150] Cum loss: 0.000592\n",
      "[15,    50] Cum loss: 0.000733\n",
      "[15,   100] Cum loss: 0.001248\n",
      "[15,   150] Cum loss: 0.001585\n",
      "[16,    50] Cum loss: 0.000631\n",
      "[16,   100] Cum loss: 0.001099\n",
      "[16,   150] Cum loss: 0.001013\n",
      "[17,    50] Cum loss: 0.000641\n",
      "[17,   100] Cum loss: 0.000874\n",
      "[17,   150] Cum loss: 0.000765\n",
      "[18,    50] Cum loss: 0.000500\n",
      "[18,   100] Cum loss: 0.000532\n",
      "[18,   150] Cum loss: 0.000469\n",
      "[19,    50] Cum loss: 0.001935\n",
      "[19,   100] Cum loss: 0.001116\n",
      "[19,   150] Cum loss: 0.000920\n",
      "[20,    50] Cum loss: 0.000457\n",
      "[20,   100] Cum loss: 0.000958\n",
      "[20,   150] Cum loss: 0.000324\n",
      "Percentage of predictions within 2 decimal places: 49.29%\n",
      "accuracy received 49.287169042769854\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m49.29    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m68.24    \u001b[0m | \u001b[0m16.0     \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 43 16 0.5\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 16\n",
      "LeNet5Blend: total params: 95368\n",
      "Train params: 0.001 0.9\n",
      "[1,    50] Cum loss: 0.247095\n",
      "[1,   100] Cum loss: 0.017262\n",
      "[1,   150] Cum loss: 0.010263\n",
      "[2,    50] Cum loss: 0.007769\n",
      "[2,   100] Cum loss: 0.003933\n",
      "[2,   150] Cum loss: 0.001847\n",
      "[3,    50] Cum loss: 0.002726\n",
      "[3,   100] Cum loss: 0.002614\n",
      "[3,   150] Cum loss: 0.002861\n",
      "[4,    50] Cum loss: 0.001216\n",
      "[4,   100] Cum loss: 0.001960\n",
      "[4,   150] Cum loss: 0.001687\n",
      "[5,    50] Cum loss: 0.001377\n",
      "[5,   100] Cum loss: 0.001226\n",
      "[5,   150] Cum loss: 0.001432\n",
      "[6,    50] Cum loss: 0.001395\n",
      "[6,   100] Cum loss: 0.001285\n",
      "[6,   150] Cum loss: 0.000621\n",
      "[7,    50] Cum loss: 0.000884\n",
      "[7,   100] Cum loss: 0.001359\n",
      "[7,   150] Cum loss: 0.000864\n",
      "[8,    50] Cum loss: 0.001302\n",
      "[8,   100] Cum loss: 0.000705\n",
      "[8,   150] Cum loss: 0.000948\n",
      "[9,    50] Cum loss: 0.001184\n",
      "[9,   100] Cum loss: 0.002759\n",
      "[9,   150] Cum loss: 0.000815\n",
      "[10,    50] Cum loss: 0.000821\n",
      "[10,   100] Cum loss: 0.001051\n",
      "[10,   150] Cum loss: 0.001383\n",
      "[11,    50] Cum loss: 0.001418\n",
      "[11,   100] Cum loss: 0.000946\n",
      "[11,   150] Cum loss: 0.000581\n",
      "[12,    50] Cum loss: 0.000789\n",
      "[12,   100] Cum loss: 0.000758\n",
      "[12,   150] Cum loss: 0.000672\n",
      "[13,    50] Cum loss: 0.000414\n",
      "[13,   100] Cum loss: 0.000999\n",
      "[13,   150] Cum loss: 0.001276\n",
      "[14,    50] Cum loss: 0.000586\n",
      "[14,   100] Cum loss: 0.000893\n",
      "[14,   150] Cum loss: 0.001085\n",
      "[15,    50] Cum loss: 0.001292\n",
      "[15,   100] Cum loss: 0.001199\n",
      "[15,   150] Cum loss: 0.000462\n",
      "[16,    50] Cum loss: 0.000668\n",
      "[16,   100] Cum loss: 0.000386\n",
      "[16,   150] Cum loss: 0.000514\n",
      "[17,    50] Cum loss: 0.000641\n",
      "[17,   100] Cum loss: 0.000419\n",
      "[17,   150] Cum loss: 0.000501\n",
      "[18,    50] Cum loss: 0.000394\n",
      "[18,   100] Cum loss: 0.000821\n",
      "[18,   150] Cum loss: 0.001035\n",
      "[19,    50] Cum loss: 0.000931\n",
      "[19,   100] Cum loss: 0.001271\n",
      "[19,   150] Cum loss: 0.000397\n",
      "[20,    50] Cum loss: 0.000883\n",
      "[20,   100] Cum loss: 0.000465\n",
      "[20,   150] Cum loss: 0.000806\n",
      "Percentage of predictions within 2 decimal places: 44.40%\n",
      "accuracy received 44.39918533604888\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m44.4     \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m43.3     \u001b[0m | \u001b[0m16.0     \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 63 8 0.0\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 8\n",
      "LeNet5Blend: total params: 54036\n",
      "Train params: 0.001 0.9\n",
      "[1,    50] Cum loss: 0.243705\n",
      "[1,   100] Cum loss: 0.041700\n",
      "[1,   150] Cum loss: 0.026938\n",
      "[2,    50] Cum loss: 0.009971\n",
      "[2,   100] Cum loss: 0.007169\n",
      "[2,   150] Cum loss: 0.007185\n",
      "[3,    50] Cum loss: 0.006431\n",
      "[3,   100] Cum loss: 0.003818\n",
      "[3,   150] Cum loss: 0.003901\n",
      "[4,    50] Cum loss: 0.004164\n",
      "[4,   100] Cum loss: 0.003375\n",
      "[4,   150] Cum loss: 0.001842\n",
      "[5,    50] Cum loss: 0.001993\n",
      "[5,   100] Cum loss: 0.003024\n",
      "[5,   150] Cum loss: 0.002535\n",
      "[6,    50] Cum loss: 0.001551\n",
      "[6,   100] Cum loss: 0.001403\n",
      "[6,   150] Cum loss: 0.001314\n",
      "[7,    50] Cum loss: 0.001229\n",
      "[7,   100] Cum loss: 0.000853\n",
      "[7,   150] Cum loss: 0.001557\n",
      "[8,    50] Cum loss: 0.001054\n",
      "[8,   100] Cum loss: 0.001320\n",
      "[8,   150] Cum loss: 0.001728\n",
      "[9,    50] Cum loss: 0.000680\n",
      "[9,   100] Cum loss: 0.000636\n",
      "[9,   150] Cum loss: 0.001342\n",
      "[10,    50] Cum loss: 0.000598\n",
      "[10,   100] Cum loss: 0.001115\n",
      "[10,   150] Cum loss: 0.001010\n",
      "[11,    50] Cum loss: 0.000889\n",
      "[11,   100] Cum loss: 0.002047\n",
      "[11,   150] Cum loss: 0.001031\n",
      "[12,    50] Cum loss: 0.000691\n",
      "[12,   100] Cum loss: 0.000677\n",
      "[12,   150] Cum loss: 0.000644\n",
      "[13,    50] Cum loss: 0.000719\n",
      "[13,   100] Cum loss: 0.001067\n",
      "[13,   150] Cum loss: 0.000479\n",
      "[14,    50] Cum loss: 0.000545\n",
      "[14,   100] Cum loss: 0.000539\n",
      "[14,   150] Cum loss: 0.000839\n",
      "[15,    50] Cum loss: 0.000836\n",
      "[15,   100] Cum loss: 0.001073\n",
      "[15,   150] Cum loss: 0.001026\n",
      "[16,    50] Cum loss: 0.000570\n",
      "[16,   100] Cum loss: 0.000539\n",
      "[16,   150] Cum loss: 0.001506\n",
      "[17,    50] Cum loss: 0.001157\n",
      "[17,   100] Cum loss: 0.000469\n",
      "[17,   150] Cum loss: 0.000330\n",
      "[18,    50] Cum loss: 0.000571\n",
      "[18,   100] Cum loss: 0.001100\n",
      "[18,   150] Cum loss: 0.000500\n",
      "[19,    50] Cum loss: 0.000317\n",
      "[19,   100] Cum loss: 0.000421\n",
      "[19,   150] Cum loss: 0.000386\n",
      "[20,    50] Cum loss: 0.000429\n",
      "[20,   100] Cum loss: 0.000569\n",
      "[20,   150] Cum loss: 0.000437\n",
      "Percentage of predictions within 2 decimal places: 15.89%\n",
      "accuracy received 15.885947046843176\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m15.89    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m63.34    \u001b[0m | \u001b[0m8.0      \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 80 16 0.0\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 16\n",
      "LeNet5Blend: total params: 99105\n",
      "Train params: 0.01 0.9\n",
      "[1,    50] Cum loss: 3.021140\n",
      "[1,   100] Cum loss: 0.035439\n",
      "[1,   150] Cum loss: 0.018755\n",
      "[2,    50] Cum loss: 0.012216\n",
      "[2,   100] Cum loss: 0.004386\n",
      "[2,   150] Cum loss: 0.004575\n",
      "[3,    50] Cum loss: 0.003877\n",
      "[3,   100] Cum loss: 0.001913\n",
      "[3,   150] Cum loss: 0.002483\n",
      "[4,    50] Cum loss: 0.001262\n",
      "[4,   100] Cum loss: 0.002915\n",
      "[4,   150] Cum loss: 0.002643\n",
      "[5,    50] Cum loss: 0.002085\n",
      "[5,   100] Cum loss: 0.002500\n",
      "[5,   150] Cum loss: 0.002007\n",
      "[6,    50] Cum loss: 0.004029\n",
      "[6,   100] Cum loss: 0.001523\n",
      "[6,   150] Cum loss: 0.001018\n",
      "[7,    50] Cum loss: 0.002994\n",
      "[7,   100] Cum loss: 0.001731\n",
      "[7,   150] Cum loss: 0.002011\n",
      "[8,    50] Cum loss: 0.000481\n",
      "[8,   100] Cum loss: 0.000943\n",
      "[8,   150] Cum loss: 0.002246\n",
      "[9,    50] Cum loss: 0.000524\n",
      "[9,   100] Cum loss: 0.002706\n",
      "[9,   150] Cum loss: 0.001544\n",
      "[10,    50] Cum loss: 0.001000\n",
      "[10,   100] Cum loss: 0.002001\n",
      "[10,   150] Cum loss: 0.000802\n",
      "[11,    50] Cum loss: 0.002135\n",
      "[11,   100] Cum loss: 0.002431\n",
      "[11,   150] Cum loss: 0.000889\n",
      "[12,    50] Cum loss: 0.000746\n",
      "[12,   100] Cum loss: 0.000900\n",
      "[12,   150] Cum loss: 0.002143\n",
      "[13,    50] Cum loss: 0.000691\n",
      "[13,   100] Cum loss: 0.000951\n",
      "[13,   150] Cum loss: 0.000468\n",
      "[14,    50] Cum loss: 0.000688\n",
      "[14,   100] Cum loss: 0.001244\n",
      "[14,   150] Cum loss: 0.001627\n",
      "[15,    50] Cum loss: 0.000974\n",
      "[15,   100] Cum loss: 0.002582\n",
      "[15,   150] Cum loss: 0.002376\n",
      "[16,    50] Cum loss: 0.000660\n",
      "[16,   100] Cum loss: 0.000513\n",
      "[16,   150] Cum loss: 0.000402\n",
      "[17,    50] Cum loss: 0.000563\n",
      "[17,   100] Cum loss: 0.001594\n",
      "[17,   150] Cum loss: 0.023304\n",
      "[18,    50] Cum loss: 0.002267\n",
      "[18,   100] Cum loss: 0.000858\n",
      "[18,   150] Cum loss: 0.000455\n",
      "[19,    50] Cum loss: 0.000375\n",
      "[19,   100] Cum loss: 0.001359\n",
      "[19,   150] Cum loss: 0.000500\n",
      "[20,    50] Cum loss: 0.000659\n",
      "[20,   100] Cum loss: 0.000283\n",
      "[20,   150] Cum loss: 0.000234\n",
      "Percentage of predictions within 2 decimal places: 52.95%\n",
      "accuracy received 52.953156822810584\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m52.95    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m80.0     \u001b[0m | \u001b[0m16.0     \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 40 8 0.40228498805606305\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 8\n",
      "LeNet5Blend: total params: 52817\n",
      "Train params: 0.009258093893698052 0.9\n",
      "[1,    50] Cum loss: 1.664746\n",
      "[1,   100] Cum loss: 0.005502\n",
      "Loss is less than 0.0001. Stopping training.\n",
      "Percentage of predictions within 2 decimal places: 100.00%\n",
      "accuracy received 100.0\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m100.0    \u001b[0m | \u001b[0m0.4023   \u001b[0m | \u001b[0m0.009258 \u001b[0m | \u001b[0m40.22    \u001b[0m | \u001b[0m8.126    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 74 16 0.5\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 16\n",
      "LeNet5Blend: total params: 98499\n",
      "Train params: 0.001 0.9\n",
      "[1,    50] Cum loss: 0.124173\n",
      "[1,   100] Cum loss: 0.014948\n",
      "[1,   150] Cum loss: 0.010178\n",
      "[2,    50] Cum loss: 0.003878\n",
      "[2,   100] Cum loss: 0.002418\n",
      "[2,   150] Cum loss: 0.001374\n",
      "[3,    50] Cum loss: 0.001524\n",
      "[3,   100] Cum loss: 0.001508\n",
      "[3,   150] Cum loss: 0.001908\n",
      "[4,    50] Cum loss: 0.001074\n",
      "[4,   100] Cum loss: 0.000715\n",
      "[4,   150] Cum loss: 0.001520\n",
      "[5,    50] Cum loss: 0.000907\n",
      "[5,   100] Cum loss: 0.001603\n",
      "[5,   150] Cum loss: 0.000739\n",
      "[6,    50] Cum loss: 0.000724\n",
      "[6,   100] Cum loss: 0.000715\n",
      "[6,   150] Cum loss: 0.000458\n",
      "[7,    50] Cum loss: 0.000407\n",
      "[7,   100] Cum loss: 0.000997\n",
      "[7,   150] Cum loss: 0.000641\n",
      "[8,    50] Cum loss: 0.003174\n",
      "[8,   100] Cum loss: 0.000989\n",
      "[8,   150] Cum loss: 0.000923\n",
      "[9,    50] Cum loss: 0.001328\n",
      "[9,   100] Cum loss: 0.001705\n",
      "[9,   150] Cum loss: 0.000661\n",
      "[10,    50] Cum loss: 0.000662\n",
      "[10,   100] Cum loss: 0.002902\n",
      "[10,   150] Cum loss: 0.000981\n",
      "[11,    50] Cum loss: 0.000491\n",
      "[11,   100] Cum loss: 0.000313\n",
      "[11,   150] Cum loss: 0.000907\n",
      "[12,    50] Cum loss: 0.000671\n",
      "[12,   100] Cum loss: 0.000312\n",
      "[12,   150] Cum loss: 0.000570\n",
      "[13,    50] Cum loss: 0.000257\n",
      "[13,   100] Cum loss: 0.001180\n",
      "[13,   150] Cum loss: 0.000831\n",
      "[14,    50] Cum loss: 0.000429\n",
      "[14,   100] Cum loss: 0.000533\n",
      "[14,   150] Cum loss: 0.000680\n",
      "[15,    50] Cum loss: 0.000376\n",
      "[15,   100] Cum loss: 0.001457\n",
      "[15,   150] Cum loss: 0.001086\n",
      "[16,    50] Cum loss: 0.000834\n",
      "[16,   100] Cum loss: 0.000484\n",
      "[16,   150] Cum loss: 0.000299\n",
      "[17,    50] Cum loss: 0.000320\n",
      "[17,   100] Cum loss: 0.000511\n",
      "[17,   150] Cum loss: 0.000591\n",
      "[18,    50] Cum loss: 0.000319\n",
      "[18,   100] Cum loss: 0.000678\n",
      "[18,   150] Cum loss: 0.000528\n",
      "[19,    50] Cum loss: 0.000467\n",
      "[19,   100] Cum loss: 0.000541\n",
      "[19,   150] Cum loss: 0.001559\n",
      "[20,    50] Cum loss: 0.000242\n",
      "[20,   100] Cum loss: 0.000389\n",
      "[20,   150] Cum loss: 0.000355\n",
      "Percentage of predictions within 2 decimal places: 28.92%\n",
      "accuracy received 28.920570264765782\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m28.92    \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m74.03    \u001b[0m | \u001b[0m16.0     \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 40 16 0.0\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 16\n",
      "LeNet5Blend: total params: 95065\n",
      "Train params: 0.01 0.9\n",
      "[1,    50] Cum loss: 1.103918\n",
      "Loss is less than 0.0001. Stopping training.\n",
      "Percentage of predictions within 2 decimal places: 0.00%\n",
      "accuracy received 0.0\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m40.0     \u001b[0m | \u001b[0m16.0     \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 42 10 0.4927893973587323\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 10\n",
      "LeNet5Blend: total params: 63509\n",
      "Train params: 0.008866245010933613 0.9\n",
      "[1,    50] Cum loss: 1.797845\n",
      "Loss is less than 0.0001. Stopping training.\n",
      "Percentage of predictions within 2 decimal places: 95.11%\n",
      "accuracy received 95.11201629327903\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m95.11    \u001b[0m | \u001b[0m0.4928   \u001b[0m | \u001b[0m0.008866 \u001b[0m | \u001b[0m42.15    \u001b[0m | \u001b[0m10.01    \u001b[0m |\n",
      "\n",
      "Convos & dropoutP: 69 8 0.0\n",
      "imgres 32 32\n",
      "H_out_1, W_out_1 31 31\n",
      "H_out_2, W_out_2 15 15\n",
      "H_out_3, W_out_4 14 13\n",
      "H_out_4, W_out_4 7 6\n",
      "outputconv2 8\n",
      "LeNet5Blend: total params: 54354\n",
      "Train params: 0.01 0.9\n",
      "[1,    50] Cum loss: 0.960825\n",
      "Loss is less than 0.0001. Stopping training.\n",
      "Percentage of predictions within 2 decimal places: 0.00%\n",
      "accuracy received 0.0\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m69.68    \u001b[0m | \u001b[0m8.0      \u001b[0m |\n",
      "=========================================================================\n",
      "Iteration 0: \n",
      "\t{'target': 100.0, 'params': {'dropout_probab': 0.208511002351287, 'learning_rate': 0.007482920440979424, 'output_conv_1': 40.0045749926938, 'output_conv_2': 10.418660581054718}}\n",
      "Iteration 1: \n",
      "\t{'target': 7.3319755600814664, 'params': {'dropout_probab': 0.07337794540855652, 'learning_rate': 0.0018310473529191804, 'output_conv_1': 47.45040845510684, 'output_conv_2': 10.764485816344383}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.0, 'params': {'dropout_probab': 0.19838373711533497, 'learning_rate': 0.005849350606030213, 'output_conv_1': 56.76778057613179, 'output_conv_2': 13.481756003174077}}\n",
      "Iteration 3: \n",
      "\t{'target': 94.5010183299389, 'params': {'dropout_probab': 0.10222612486575872, 'learning_rate': 0.00890305692751851, 'output_conv_1': 41.09550372791705, 'output_conv_2': 13.363740081427217}}\n",
      "Iteration 4: \n",
      "\t{'target': 89.0020366598778, 'params': {'dropout_probab': 0.20865240118356349, 'learning_rate': 0.006028208456011766, 'output_conv_1': 45.61547754380935, 'output_conv_2': 9.58481191267903}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.0, 'params': {'dropout_probab': 0.40037228433776834, 'learning_rate': 0.00971435418147458, 'output_conv_1': 52.53696712636972, 'output_conv_2': 13.538580925354513}}\n",
      "Iteration 6: \n",
      "\t{'target': 100.0, 'params': {'dropout_probab': 0.43819457614801915, 'learning_rate': 0.009051459971534628, 'output_conv_1': 43.40176845479112, 'output_conv_2': 8.312438265863058}}\n",
      "Iteration 7: \n",
      "\t{'target': 100.0, 'params': {'dropout_probab': 0.08491520978228445, 'learning_rate': 0.008903282530864719, 'output_conv_1': 43.93387335332201, 'output_conv_2': 11.368861000040418}}\n",
      "Iteration 8: \n",
      "\t{'target': 72.30142566191446, 'params': {'dropout_probab': 0.47894476507525097, 'learning_rate': 0.005798487564757154, 'output_conv_1': 67.67508455801894, 'output_conv_2': 10.524125048048504}}\n",
      "Iteration 9: \n",
      "\t{'target': 100.0, 'params': {'dropout_probab': 0.34325046384079183, 'learning_rate': 0.008511631047076357, 'output_conv_1': 40.73153109376767, 'output_conv_2': 14.00115451955974}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.0, 'params': {'dropout_probab': 0.5, 'learning_rate': 0.0067450735094932565, 'output_conv_1': 75.85440903141287, 'output_conv_2': 8.361958023474198}}\n",
      "Iteration 11: \n",
      "\t{'target': 49.287169042769854, 'params': {'dropout_probab': 0.0, 'learning_rate': 0.001, 'output_conv_1': 68.23968244914768, 'output_conv_2': 16.0}}\n",
      "Iteration 12: \n",
      "\t{'target': 44.39918533604888, 'params': {'dropout_probab': 0.5, 'learning_rate': 0.001, 'output_conv_1': 43.29888398820324, 'output_conv_2': 16.0}}\n",
      "Iteration 13: \n",
      "\t{'target': 15.885947046843176, 'params': {'dropout_probab': 0.0, 'learning_rate': 0.001, 'output_conv_1': 63.34257051865002, 'output_conv_2': 8.0}}\n",
      "Iteration 14: \n",
      "\t{'target': 52.953156822810584, 'params': {'dropout_probab': 0.0, 'learning_rate': 0.01, 'output_conv_1': 80.0, 'output_conv_2': 16.0}}\n",
      "Iteration 15: \n",
      "\t{'target': 100.0, 'params': {'dropout_probab': 0.40228498805606305, 'learning_rate': 0.009258093893698052, 'output_conv_1': 40.21547048066022, 'output_conv_2': 8.126086207970754}}\n",
      "Iteration 16: \n",
      "\t{'target': 28.920570264765782, 'params': {'dropout_probab': 0.5, 'learning_rate': 0.001, 'output_conv_1': 74.02925991695439, 'output_conv_2': 16.0}}\n",
      "Iteration 17: \n",
      "\t{'target': 0.0, 'params': {'dropout_probab': 0.0, 'learning_rate': 0.01, 'output_conv_1': 40.0, 'output_conv_2': 16.0}}\n",
      "Iteration 18: \n",
      "\t{'target': 95.11201629327903, 'params': {'dropout_probab': 0.4927893973587323, 'learning_rate': 0.008866245010933613, 'output_conv_1': 42.15144705663414, 'output_conv_2': 10.00567360162201}}\n",
      "Iteration 19: \n",
      "\t{'target': 0.0, 'params': {'dropout_probab': 0.0, 'learning_rate': 0.01, 'output_conv_1': 69.68161146202203, 'output_conv_2': 8.0}}\n",
      "{'target': 100.0, 'params': {'dropout_probab': 0.208511002351287, 'learning_rate': 0.007482920440979424, 'output_conv_1': 40.0045749926938, 'output_conv_2': 10.418660581054718}}\n"
     ]
    }
   ],
   "source": [
    "Optimize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
